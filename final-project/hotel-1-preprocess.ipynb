{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import gensim,logging\n",
    "from gensim.parsing import PorterStemmer\n",
    "from gensim.models import Word2Vec, Doc2Vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import nltk\n",
    "import pickle\n",
    "import warnings\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "np.random.seed(0)\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pd.read_csv('./input/stopwords.csv',names=['stop'])\n",
    "new_stop = stop_words.stop.map(lambda x: str.capitalize(x))\n",
    "all_stop_set = set(stop_words.stop.append(new_stop,ignore_index=True))\n",
    "\n",
    "#generate combinations of all_stop_words, bigrams\n",
    "#generate bigram\n",
    "bigram_stops=[]\n",
    "for a in all_stop_set:\n",
    "    for b in all_stop_set:\n",
    "        bigram_stop = a+\"_\"+b\n",
    "        bigram_stops.append(bigram_stop)\n",
    "all_stop_set = all_stop_set.union(bigram_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/deceptive-opinion.csv')\n",
    "df2 = pd.read_csv('./input/Hotel_Reviews.csv')\n",
    "data2 = df2[~df2['lat'].isnull()]\n",
    "data2 = data2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unlabelled = []\n",
    "for i,line in enumerate(data2['Negative_Review']):\n",
    "    inp = \"\"\n",
    "    if (np.random.randint(0,2) == 0):\n",
    "        inp = data2.loc[i,'Negative_Review'] + ' ' + data2.loc[i,'Positive_Review']\n",
    "    else:\n",
    "        inp = data2.loc[i,'Positive_Review'] + ' ' + data2.loc[i,'Negative_Review']\n",
    "    all_unlabelled.append(inp)\n",
    "\n",
    "data2['All_Text'] = pd.Series(all_unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[['All_Text','Negative_Review','Positive_Review']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate truth and fake df\n",
    "truedf = df[df.deceptive=='truthful'].loc[:,'text']\n",
    "fakedf = df[df.deceptive=='deceptive'].loc[:,'text']\n",
    "truedfy = df[df.deceptive=='truthful'].loc[:,'deceptive']\n",
    "fakedfy = df[df.deceptive=='deceptive'].loc[:,'deceptive']\n",
    "truedfy.replace({'truthful':1},inplace=True)\n",
    "fakedfy.replace({'deceptive':0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(sentdf, tokens_only=False):\n",
    "    for i, line in enumerate(sentdf):\n",
    "        if tokens_only:\n",
    "            yield list(gensim.utils.tokenize(line))\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(list(gensim.utils.tokenize(line)),[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def to_list(doc):\n",
    "    return [t.text for t in doc]\n",
    "\n",
    "# true_corpus = [to_list(nlp(s))[:-1] for ind,s in enumerate(truedf)]\n",
    "# fake_corpus = [to_list(nlp(s))[:-1] for ind,s in enumerate(fakedf)]\n",
    "\n",
    "# temp_arr = []\n",
    "# for ind,s in enumerate(data2['All_Text']):\n",
    "#     if ind % 100 == 0:\n",
    "#         print (ind)\n",
    "#     temp_arr.append(to_list(nlp(s))[:-1])\n",
    "# unlabelled_corpus = temp_arr.copy()\n",
    "true_corpus = list(read_corpus(truedf,tokens_only=True))\n",
    "fake_corpus = list(read_corpus(fakedf,tokens_only=True))\n",
    "unlabelled_corpus = list(read_corpus(data2['All_Text'], tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unlabelled_corpus, open('./input/unlabelled_corpus_raw.p','wb'))\n",
    "pickle.dump(fake_corpus, open('./input/fake_corpus_raw.p','wb'))\n",
    "pickle.dump(true_corpus, open('./input/true_corpus_raw.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_corpus = pickle.load(open('./input/unlabelled_corpus_raw.p','rb'))\n",
    "true_corpus = pickle.load(open('./input/true_corpus_raw.p','rb'))\n",
    "fake_corpus = pickle.load(open('./input/fake_corpus_raw.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-28 23:41:07,606 : INFO : collecting all words and their counts\n",
      "2018-01-28 23:41:07,607 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-28 23:41:08,040 : INFO : PROGRESS: at sentence #10000, processed 381532 words and 151085 word types\n",
      "2018-01-28 23:41:08,417 : INFO : PROGRESS: at sentence #20000, processed 691920 words and 249660 word types\n",
      "2018-01-28 23:41:08,890 : INFO : PROGRESS: at sentence #30000, processed 1023549 words and 348766 word types\n",
      "2018-01-28 23:41:09,336 : INFO : PROGRESS: at sentence #40000, processed 1357244 words and 442359 word types\n",
      "2018-01-28 23:41:09,764 : INFO : PROGRESS: at sentence #50000, processed 1696278 words and 532372 word types\n",
      "2018-01-28 23:41:10,201 : INFO : PROGRESS: at sentence #60000, processed 2018082 words and 617030 word types\n",
      "2018-01-28 23:41:10,701 : INFO : PROGRESS: at sentence #70000, processed 2359057 words and 707817 word types\n",
      "2018-01-28 23:41:11,150 : INFO : PROGRESS: at sentence #80000, processed 2694422 words and 791130 word types\n",
      "2018-01-28 23:41:11,594 : INFO : PROGRESS: at sentence #90000, processed 3018990 words and 868982 word types\n",
      "2018-01-28 23:41:12,035 : INFO : PROGRESS: at sentence #100000, processed 3360463 words and 951560 word types\n",
      "2018-01-28 23:41:12,459 : INFO : PROGRESS: at sentence #110000, processed 3693480 words and 1032186 word types\n",
      "2018-01-28 23:41:12,884 : INFO : PROGRESS: at sentence #120000, processed 4044074 words and 1115136 word types\n",
      "2018-01-28 23:41:13,303 : INFO : PROGRESS: at sentence #130000, processed 4389099 words and 1192082 word types\n",
      "2018-01-28 23:41:13,734 : INFO : PROGRESS: at sentence #140000, processed 4742291 words and 1272079 word types\n",
      "2018-01-28 23:41:14,139 : INFO : PROGRESS: at sentence #150000, processed 5073711 words and 1342857 word types\n",
      "2018-01-28 23:41:14,610 : INFO : PROGRESS: at sentence #160000, processed 5400404 words and 1412134 word types\n",
      "2018-01-28 23:41:15,002 : INFO : PROGRESS: at sentence #170000, processed 5723375 words and 1480584 word types\n",
      "2018-01-28 23:41:15,401 : INFO : PROGRESS: at sentence #180000, processed 6048936 words and 1546919 word types\n",
      "2018-01-28 23:41:15,824 : INFO : PROGRESS: at sentence #190000, processed 6392150 words and 1619731 word types\n",
      "2018-01-28 23:41:16,235 : INFO : PROGRESS: at sentence #200000, processed 6733267 words and 1690568 word types\n",
      "2018-01-28 23:41:16,665 : INFO : PROGRESS: at sentence #210000, processed 7074442 words and 1761566 word types\n",
      "2018-01-28 23:41:17,121 : INFO : PROGRESS: at sentence #220000, processed 7415330 words and 1830679 word types\n",
      "2018-01-28 23:41:17,585 : INFO : PROGRESS: at sentence #230000, processed 7743819 words and 1895868 word types\n",
      "2018-01-28 23:41:18,006 : INFO : PROGRESS: at sentence #240000, processed 8079151 words and 1961053 word types\n",
      "2018-01-28 23:41:18,429 : INFO : PROGRESS: at sentence #250000, processed 8419066 words and 2029045 word types\n",
      "2018-01-28 23:41:18,855 : INFO : PROGRESS: at sentence #260000, processed 8761160 words and 2094799 word types\n",
      "2018-01-28 23:41:19,271 : INFO : PROGRESS: at sentence #270000, processed 9099380 words and 2161426 word types\n",
      "2018-01-28 23:41:19,725 : INFO : PROGRESS: at sentence #280000, processed 9466883 words and 2236970 word types\n",
      "2018-01-28 23:41:20,183 : INFO : PROGRESS: at sentence #290000, processed 9822096 words and 2306910 word types\n",
      "2018-01-28 23:41:20,580 : INFO : PROGRESS: at sentence #300000, processed 10141722 words and 2368967 word types\n",
      "2018-01-28 23:41:20,998 : INFO : PROGRESS: at sentence #310000, processed 10480892 words and 2435971 word types\n",
      "2018-01-28 23:41:21,506 : INFO : PROGRESS: at sentence #320000, processed 10827476 words and 2504199 word types\n",
      "2018-01-28 23:41:21,973 : INFO : PROGRESS: at sentence #330000, processed 11167259 words and 2570145 word types\n",
      "2018-01-28 23:41:22,401 : INFO : PROGRESS: at sentence #340000, processed 11504399 words and 2634697 word types\n",
      "2018-01-28 23:41:22,876 : INFO : PROGRESS: at sentence #350000, processed 11841612 words and 2698442 word types\n",
      "2018-01-28 23:41:23,392 : INFO : PROGRESS: at sentence #360000, processed 12185088 words and 2763919 word types\n",
      "2018-01-28 23:41:23,977 : INFO : PROGRESS: at sentence #370000, processed 12516982 words and 2826402 word types\n",
      "2018-01-28 23:41:24,399 : INFO : PROGRESS: at sentence #380000, processed 12864815 words and 2891790 word types\n",
      "2018-01-28 23:41:24,819 : INFO : PROGRESS: at sentence #390000, processed 13201725 words and 2952656 word types\n",
      "2018-01-28 23:41:25,233 : INFO : PROGRESS: at sentence #400000, processed 13539384 words and 3015654 word types\n",
      "2018-01-28 23:41:25,639 : INFO : PROGRESS: at sentence #410000, processed 13878075 words and 3079447 word types\n",
      "2018-01-28 23:41:26,077 : INFO : PROGRESS: at sentence #420000, processed 14241877 words and 3148353 word types\n",
      "2018-01-28 23:41:26,515 : INFO : PROGRESS: at sentence #430000, processed 14600630 words and 3213593 word types\n",
      "2018-01-28 23:41:26,951 : INFO : PROGRESS: at sentence #440000, processed 14960106 words and 3277269 word types\n",
      "2018-01-28 23:41:27,335 : INFO : PROGRESS: at sentence #450000, processed 15271394 words and 3332095 word types\n",
      "2018-01-28 23:41:27,757 : INFO : PROGRESS: at sentence #460000, processed 15622687 words and 3393442 word types\n",
      "2018-01-28 23:41:28,159 : INFO : PROGRESS: at sentence #470000, processed 15950723 words and 3450416 word types\n",
      "2018-01-28 23:41:28,572 : INFO : PROGRESS: at sentence #480000, processed 16291950 words and 3513202 word types\n",
      "2018-01-28 23:41:28,985 : INFO : PROGRESS: at sentence #490000, processed 16630040 words and 3573752 word types\n",
      "2018-01-28 23:41:29,417 : INFO : PROGRESS: at sentence #500000, processed 16983304 words and 3636146 word types\n",
      "2018-01-28 23:41:29,860 : INFO : PROGRESS: at sentence #510000, processed 17349557 words and 3701966 word types\n",
      "2018-01-28 23:41:30,233 : INFO : collected 3770377 word types from a corpus of 17662986 words (unigram + bigrams) and 514070 sentences\n",
      "2018-01-28 23:41:30,235 : INFO : using 3770377 counts as vocab in Phrases<0 vocab, min_count=2, threshold=2, max_vocab_size=40000000>\n",
      "2018-01-28 23:41:30,239 : INFO : source_vocab length 3770377\n",
      "2018-01-28 23:41:37,089 : INFO : Phraser added 50000 phrasegrams\n",
      "2018-01-28 23:42:13,546 : INFO : Phraser built with 88991 88991 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "sentence_stream = [s for s in unlabelled_corpus + true_corpus + fake_corpus]\n",
    "phrases = Phrases(sentence_stream, threshold=2, min_count=2, common_terms = list(all_stop_set))\n",
    "bigram = Phraser(phrases)\n",
    "sentence_stream = [bigram[s] for s in sentence_stream]\n",
    "true_corpus = [bigram[s] for s in true_corpus]\n",
    "fake_corpus = [bigram[s] for s in fake_corpus]\n",
    "unlabelled_corpus = [bigram[s] for s in unlabelled_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My',\n",
       "  'husband_and_I_visited',\n",
       "  'the',\n",
       "  'Fairmont_Chicago',\n",
       "  'Millennium_Park',\n",
       "  'for',\n",
       "  'our',\n",
       "  'honeymoon',\n",
       "  'The',\n",
       "  'customer_service',\n",
       "  'was',\n",
       "  'amazing',\n",
       "  'From',\n",
       "  'the',\n",
       "  'time',\n",
       "  'we',\n",
       "  'booked',\n",
       "  'our',\n",
       "  'packege',\n",
       "  'to',\n",
       "  'the',\n",
       "  'time',\n",
       "  'we',\n",
       "  'checked',\n",
       "  'in',\n",
       "  'everything',\n",
       "  'was',\n",
       "  'absolutely_amazing',\n",
       "  'These',\n",
       "  'people',\n",
       "  'were',\n",
       "  'proficient',\n",
       "  'respectful',\n",
       "  'and',\n",
       "  'very',\n",
       "  'thoughtful',\n",
       "  'The',\n",
       "  'Fairmont',\n",
       "  'had',\n",
       "  'a',\n",
       "  'lounge',\n",
       "  'a',\n",
       "  'wine',\n",
       "  'room',\n",
       "  'a',\n",
       "  'bar',\n",
       "  'and',\n",
       "  'a',\n",
       "  'restaurant',\n",
       "  'I',\n",
       "  'couldn_t',\n",
       "  'decide',\n",
       "  'where',\n",
       "  'I',\n",
       "  'wanted_to_go',\n",
       "  'first',\n",
       "  'After',\n",
       "  'we',\n",
       "  'put_our_bags',\n",
       "  'up',\n",
       "  'we',\n",
       "  'headed',\n",
       "  'down',\n",
       "  'to',\n",
       "  'the',\n",
       "  'wine',\n",
       "  'room',\n",
       "  'It',\n",
       "  'was',\n",
       "  'totally',\n",
       "  'delicious',\n",
       "  'We',\n",
       "  'also',\n",
       "  'got',\n",
       "  'free_wine',\n",
       "  'just',\n",
       "  'because',\n",
       "  'it',\n",
       "  'was',\n",
       "  'our',\n",
       "  'honeymoon',\n",
       "  'Then',\n",
       "  'after',\n",
       "  'a',\n",
       "  'few',\n",
       "  'glasses_of_wine',\n",
       "  'we',\n",
       "  'hit',\n",
       "  'the',\n",
       "  'spa',\n",
       "  'once',\n",
       "  'again',\n",
       "  'excellent',\n",
       "  'Everything',\n",
       "  'smells_like',\n",
       "  'honeysuckle',\n",
       "  'and',\n",
       "  'everyone_smiles',\n",
       "  'all',\n",
       "  'the',\n",
       "  'time',\n",
       "  'We',\n",
       "  'stayed',\n",
       "  'in',\n",
       "  'the',\n",
       "  'gold',\n",
       "  'room',\n",
       "  'Although',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'little_bit',\n",
       "  'smaller_than_I_thought',\n",
       "  'it',\n",
       "  'would',\n",
       "  'be',\n",
       "  'I',\n",
       "  'was',\n",
       "  'definitely',\n",
       "  'satisfied',\n",
       "  'with',\n",
       "  'the',\n",
       "  'huge_king',\n",
       "  'bed',\n",
       "  'with',\n",
       "  'even',\n",
       "  'bigger',\n",
       "  'pillows',\n",
       "  'My',\n",
       "  'husband',\n",
       "  'and',\n",
       "  'I',\n",
       "  'relaxed',\n",
       "  'in',\n",
       "  'fluffy_white',\n",
       "  'bath_robes',\n",
       "  'while',\n",
       "  'we',\n",
       "  'sipped',\n",
       "  'champagne',\n",
       "  'while',\n",
       "  'we',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'sparkling',\n",
       "  'lights',\n",
       "  'of',\n",
       "  'the',\n",
       "  'city',\n",
       "  'It',\n",
       "  'was',\n",
       "  'a',\n",
       "  'wonderful_experience',\n",
       "  'that',\n",
       "  'I',\n",
       "  'will_never',\n",
       "  'forget',\n",
       "  'Four',\n",
       "  'thumbs',\n",
       "  'up'],\n",
       " ['My',\n",
       "  'wife',\n",
       "  'and',\n",
       "  'I',\n",
       "  'booked_a_Deluxe',\n",
       "  'Accessible',\n",
       "  'Room',\n",
       "  'at',\n",
       "  'this',\n",
       "  'beautiful',\n",
       "  'hotel',\n",
       "  'for',\n",
       "  'three_nights',\n",
       "  'The',\n",
       "  'wonderful',\n",
       "  'photos_don',\n",
       "  't',\n",
       "  'do',\n",
       "  'the',\n",
       "  'hotel_justice',\n",
       "  'It',\n",
       "  'couldn_t',\n",
       "  'have',\n",
       "  'been',\n",
       "  'a',\n",
       "  'more',\n",
       "  'ideal',\n",
       "  'stay',\n",
       "  'The',\n",
       "  'room',\n",
       "  'was',\n",
       "  'spacious',\n",
       "  'and',\n",
       "  'and',\n",
       "  'clean',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  'sparkling',\n",
       "  'with',\n",
       "  'every_amenity',\n",
       "  'one',\n",
       "  'could',\n",
       "  'imagine',\n",
       "  'The',\n",
       "  'bed',\n",
       "  'felt_like',\n",
       "  'it',\n",
       "  'had',\n",
       "  'been',\n",
       "  'just',\n",
       "  'delivered',\n",
       "  'Our',\n",
       "  'views',\n",
       "  'of',\n",
       "  'downtown_Chicago',\n",
       "  'were',\n",
       "  'spectacular',\n",
       "  'On',\n",
       "  'top',\n",
       "  'of',\n",
       "  'all',\n",
       "  'of',\n",
       "  'this',\n",
       "  'the',\n",
       "  'hotel',\n",
       "  'staff',\n",
       "  'was',\n",
       "  'extremely_helpful',\n",
       "  'They',\n",
       "  'explained',\n",
       "  'the',\n",
       "  'best_routes',\n",
       "  'to',\n",
       "  'the',\n",
       "  'restaurants',\n",
       "  'we',\n",
       "  'wanted_to_eat',\n",
       "  'at',\n",
       "  'and',\n",
       "  'they',\n",
       "  'even',\n",
       "  'went',\n",
       "  'as',\n",
       "  'far',\n",
       "  'as',\n",
       "  'securing',\n",
       "  'us',\n",
       "  'reservations',\n",
       "  'for',\n",
       "  'the',\n",
       "  'places',\n",
       "  'they',\n",
       "  'knew',\n",
       "  'would',\n",
       "  'be',\n",
       "  'busy',\n",
       "  'Believe',\n",
       "  'everything',\n",
       "  'good',\n",
       "  'that',\n",
       "  'you',\n",
       "  'read',\n",
       "  'If',\n",
       "  'you',\n",
       "  'visit_Chicago',\n",
       "  'stay_at_the_Conrad',\n",
       "  'Chicago'],\n",
       " ['Quite_simply',\n",
       "  'the',\n",
       "  'Hyatt_Regency',\n",
       "  'Chicago',\n",
       "  'is',\n",
       "  'the',\n",
       "  'business_traveler',\n",
       "  's',\n",
       "  'best_friend',\n",
       "  'Recently',\n",
       "  'I',\n",
       "  'had',\n",
       "  'to',\n",
       "  'attend',\n",
       "  'a',\n",
       "  'seminar',\n",
       "  'in',\n",
       "  'Chicago',\n",
       "  'and',\n",
       "  'stayed_at_the_Hyatt',\n",
       "  'To',\n",
       "  'say',\n",
       "  'I',\n",
       "  'was',\n",
       "  'impressed',\n",
       "  'would',\n",
       "  'be',\n",
       "  'an',\n",
       "  'understatement',\n",
       "  'Centrally_located',\n",
       "  'in',\n",
       "  'the',\n",
       "  'heart_of_downtown',\n",
       "  'the',\n",
       "  'Hyatt',\n",
       "  'has',\n",
       "  'perfected',\n",
       "  'the',\n",
       "  'art',\n",
       "  'of',\n",
       "  'ultimate',\n",
       "  'guest_satisfaction',\n",
       "  'On',\n",
       "  'arrival',\n",
       "  'the',\n",
       "  'staff',\n",
       "  'goes',\n",
       "  'into',\n",
       "  'overdrive',\n",
       "  'They',\n",
       "  'make_you_feel',\n",
       "  'like',\n",
       "  'you',\n",
       "  'are',\n",
       "  'the',\n",
       "  'only',\n",
       "  'guest',\n",
       "  'in',\n",
       "  'this',\n",
       "  'huge',\n",
       "  'hotel',\n",
       "  'The',\n",
       "  'rooms',\n",
       "  'are',\n",
       "  'very',\n",
       "  'big',\n",
       "  'and',\n",
       "  'elegantly_furnished',\n",
       "  'with',\n",
       "  'percent',\n",
       "  'cotton',\n",
       "  'linens',\n",
       "  'My',\n",
       "  'room',\n",
       "  'came',\n",
       "  'equipped',\n",
       "  'with',\n",
       "  'flat_screen',\n",
       "  'tv',\n",
       "  'wi_fi',\n",
       "  'and',\n",
       "  'high_speed',\n",
       "  'internet',\n",
       "  'Room_service',\n",
       "  'was',\n",
       "  'quite_extensive',\n",
       "  'and',\n",
       "  'if',\n",
       "  'it',\n",
       "  'was',\n",
       "  'not',\n",
       "  'on',\n",
       "  'the',\n",
       "  'menu',\n",
       "  'they',\n",
       "  'went_out_of_their_way',\n",
       "  'to',\n",
       "  'accomodate',\n",
       "  'you',\n",
       "  'There',\n",
       "  'were',\n",
       "  'also',\n",
       "  'car_rental',\n",
       "  'on',\n",
       "  'site',\n",
       "  'as',\n",
       "  'were',\n",
       "  'restaurants',\n",
       "  'cocktail_bar',\n",
       "  'and',\n",
       "  'a',\n",
       "  'hour',\n",
       "  'dining_room',\n",
       "  'They',\n",
       "  'have',\n",
       "  'state_of_the_art',\n",
       "  'conference',\n",
       "  'rooms',\n",
       "  'with',\n",
       "  'high_tech',\n",
       "  'features',\n",
       "  'There',\n",
       "  'is',\n",
       "  'also',\n",
       "  'catering',\n",
       "  'and',\n",
       "  'limo_service',\n",
       "  'The',\n",
       "  'staff',\n",
       "  'is',\n",
       "  'multilingual',\n",
       "  'and',\n",
       "  'the',\n",
       "  'concierge_service',\n",
       "  'is',\n",
       "  'very',\n",
       "  'professional',\n",
       "  'All',\n",
       "  'in',\n",
       "  'all',\n",
       "  'they',\n",
       "  'will',\n",
       "  'pamper',\n",
       "  'you',\n",
       "  'from',\n",
       "  'head_to_toe',\n",
       "  'and',\n",
       "  'other',\n",
       "  'parts',\n",
       "  'in',\n",
       "  'between',\n",
       "  'This',\n",
       "  'hotel',\n",
       "  'is',\n",
       "  'definitely_worth',\n",
       "  'the',\n",
       "  'price',\n",
       "  'and',\n",
       "  'I',\n",
       "  'will_definitely',\n",
       "  'return',\n",
       "  'not',\n",
       "  'as',\n",
       "  'a',\n",
       "  'business_traveler',\n",
       "  'but',\n",
       "  'as',\n",
       "  'a',\n",
       "  'tourist',\n",
       "  'who',\n",
       "  'wants',\n",
       "  'to',\n",
       "  'be',\n",
       "  'pampered',\n",
       "  'My',\n",
       "  'only',\n",
       "  'regret',\n",
       "  'That',\n",
       "  'I',\n",
       "  'could',\n",
       "  'not',\n",
       "  'stay',\n",
       "  'much_longer']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-28 23:43:24,266 : INFO : collecting all words and their counts\n",
      "2018-01-28 23:43:24,268 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-28 23:43:24,628 : INFO : PROGRESS: at sentence #10000, processed 326563 words and 162988 word types\n",
      "2018-01-28 23:43:24,968 : INFO : PROGRESS: at sentence #20000, processed 591911 words and 275196 word types\n",
      "2018-01-28 23:43:25,345 : INFO : PROGRESS: at sentence #30000, processed 875085 words and 390240 word types\n",
      "2018-01-28 23:43:25,696 : INFO : PROGRESS: at sentence #40000, processed 1159890 words and 500717 word types\n",
      "2018-01-28 23:43:26,040 : INFO : PROGRESS: at sentence #50000, processed 1448203 words and 608822 word types\n",
      "2018-01-28 23:43:26,408 : INFO : PROGRESS: at sentence #60000, processed 1722727 words and 710410 word types\n",
      "2018-01-28 23:43:26,756 : INFO : PROGRESS: at sentence #70000, processed 2015693 words and 818026 word types\n",
      "2018-01-28 23:43:27,105 : INFO : PROGRESS: at sentence #80000, processed 2303198 words and 918673 word types\n",
      "2018-01-28 23:43:27,439 : INFO : PROGRESS: at sentence #90000, processed 2580448 words and 1013829 word types\n",
      "2018-01-28 23:43:27,785 : INFO : PROGRESS: at sentence #100000, processed 2872573 words and 1114858 word types\n",
      "2018-01-28 23:43:28,148 : INFO : PROGRESS: at sentence #110000, processed 3158032 words and 1213080 word types\n",
      "2018-01-28 23:43:28,527 : INFO : PROGRESS: at sentence #120000, processed 3458337 words and 1314947 word types\n",
      "2018-01-28 23:43:28,993 : INFO : PROGRESS: at sentence #130000, processed 3753838 words and 1410847 word types\n",
      "2018-01-28 23:43:29,374 : INFO : PROGRESS: at sentence #140000, processed 4056244 words and 1510234 word types\n",
      "2018-01-28 23:43:29,706 : INFO : PROGRESS: at sentence #150000, processed 4339000 words and 1599188 word types\n",
      "2018-01-28 23:43:30,055 : INFO : PROGRESS: at sentence #160000, processed 4618671 words and 1686237 word types\n",
      "2018-01-28 23:43:30,394 : INFO : PROGRESS: at sentence #170000, processed 4895498 words and 1772448 word types\n",
      "2018-01-28 23:43:30,755 : INFO : PROGRESS: at sentence #180000, processed 5173402 words and 1856321 word types\n",
      "2018-01-28 23:43:31,157 : INFO : PROGRESS: at sentence #190000, processed 5467041 words and 1948051 word types\n",
      "2018-01-28 23:43:31,527 : INFO : PROGRESS: at sentence #200000, processed 5759616 words and 2037409 word types\n",
      "2018-01-28 23:43:31,885 : INFO : PROGRESS: at sentence #210000, processed 6052296 words and 2127134 word types\n",
      "2018-01-28 23:43:32,249 : INFO : PROGRESS: at sentence #220000, processed 6344293 words and 2214999 word types\n",
      "2018-01-28 23:43:32,610 : INFO : PROGRESS: at sentence #230000, processed 6623822 words and 2298738 word types\n",
      "2018-01-28 23:43:32,976 : INFO : PROGRESS: at sentence #240000, processed 6909124 words and 2383003 word types\n",
      "2018-01-28 23:43:33,338 : INFO : PROGRESS: at sentence #250000, processed 7201203 words and 2469718 word types\n",
      "2018-01-28 23:43:33,707 : INFO : PROGRESS: at sentence #260000, processed 7492875 words and 2555186 word types\n",
      "2018-01-28 23:43:34,088 : INFO : PROGRESS: at sentence #270000, processed 7781849 words and 2640997 word types\n",
      "2018-01-28 23:43:34,483 : INFO : PROGRESS: at sentence #280000, processed 8095966 words and 2738026 word types\n",
      "2018-01-28 23:43:35,006 : INFO : PROGRESS: at sentence #290000, processed 8398800 words and 2828591 word types\n",
      "2018-01-28 23:43:35,332 : INFO : PROGRESS: at sentence #300000, processed 8672162 words and 2908709 word types\n",
      "2018-01-28 23:43:35,683 : INFO : PROGRESS: at sentence #310000, processed 8963300 words and 2995014 word types\n",
      "2018-01-28 23:43:36,039 : INFO : PROGRESS: at sentence #320000, processed 9259778 words and 3083113 word types\n",
      "2018-01-28 23:43:36,389 : INFO : PROGRESS: at sentence #330000, processed 9550102 words and 3168809 word types\n",
      "2018-01-28 23:43:36,736 : INFO : PROGRESS: at sentence #340000, processed 9839742 words and 3252485 word types\n",
      "2018-01-28 23:43:37,076 : INFO : PROGRESS: at sentence #350000, processed 10128041 words and 3336040 word types\n",
      "2018-01-28 23:43:37,442 : INFO : PROGRESS: at sentence #360000, processed 10422012 words and 3421359 word types\n",
      "2018-01-28 23:43:37,794 : INFO : PROGRESS: at sentence #370000, processed 10706481 words and 3503040 word types\n",
      "2018-01-28 23:43:38,197 : INFO : PROGRESS: at sentence #380000, processed 11004235 words and 3588608 word types\n",
      "2018-01-28 23:43:38,570 : INFO : PROGRESS: at sentence #390000, processed 11293014 words and 3668868 word types\n",
      "2018-01-28 23:43:38,935 : INFO : PROGRESS: at sentence #400000, processed 11582757 words and 3751177 word types\n",
      "2018-01-28 23:43:39,295 : INFO : PROGRESS: at sentence #410000, processed 11873828 words and 3834880 word types\n",
      "2018-01-28 23:43:39,675 : INFO : PROGRESS: at sentence #420000, processed 12186761 words and 3924715 word types\n",
      "2018-01-28 23:43:40,047 : INFO : PROGRESS: at sentence #430000, processed 12494207 words and 4010905 word types\n",
      "2018-01-28 23:43:40,442 : INFO : PROGRESS: at sentence #440000, processed 12801876 words and 4095255 word types\n",
      "2018-01-28 23:43:40,784 : INFO : PROGRESS: at sentence #450000, processed 13067565 words and 4168648 word types\n",
      "2018-01-28 23:43:41,158 : INFO : PROGRESS: at sentence #460000, processed 13368156 words and 4249881 word types\n",
      "2018-01-28 23:43:41,495 : INFO : PROGRESS: at sentence #470000, processed 13648226 words and 4325911 word types\n",
      "2018-01-28 23:43:41,860 : INFO : PROGRESS: at sentence #480000, processed 13941651 words and 4408580 word types\n",
      "2018-01-28 23:43:42,222 : INFO : PROGRESS: at sentence #490000, processed 14232475 words and 4488786 word types\n",
      "2018-01-28 23:43:42,584 : INFO : PROGRESS: at sentence #500000, processed 14535585 words and 4571975 word types\n",
      "2018-01-28 23:43:42,982 : INFO : PROGRESS: at sentence #510000, processed 14851111 words and 4658022 word types\n",
      "2018-01-28 23:43:43,332 : INFO : collected 4741411 word types from a corpus of 15124870 words (unigram + bigrams) and 514070 sentences\n",
      "2018-01-28 23:43:43,334 : INFO : using 4741411 counts as vocab in Phrases<0 vocab, min_count=2, threshold=2, max_vocab_size=40000000>\n",
      "2018-01-28 23:43:43,434 : INFO : source_vocab length 4741411\n",
      "2018-01-28 23:43:46,827 : INFO : Phraser added 50000 phrasegrams\n",
      "2018-01-28 23:43:51,377 : INFO : Phraser added 100000 phrasegrams\n",
      "2018-01-28 23:43:58,195 : INFO : Phraser added 150000 phrasegrams\n",
      "2018-01-28 23:44:10,073 : INFO : Phraser added 200000 phrasegrams\n",
      "2018-01-28 23:44:45,507 : INFO : Phraser added 250000 phrasegrams\n",
      "2018-01-28 23:44:58,624 : INFO : Phraser built with 252026 252026 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sentence_stream, threshold=2, min_count=2,common_terms = list(all_stop_set))\n",
    "trigram = Phraser(phrases)\n",
    "sentence_stream = [trigram[s] for s in sentence_stream]\n",
    "true_corpus = [trigram[s] for s in true_corpus]\n",
    "fake_corpus = [trigram[s] for s in fake_corpus]\n",
    "unlabelled_corpus = [trigram[s] for s in unlabelled_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Omni_Chicago',\n",
       "  'really',\n",
       "  'delivers',\n",
       "  'on',\n",
       "  'all',\n",
       "  'fronts',\n",
       "  'from',\n",
       "  'the',\n",
       "  'spaciousness_of_the_rooms',\n",
       "  'to',\n",
       "  'the',\n",
       "  'helpful_staff',\n",
       "  'to',\n",
       "  'the',\n",
       "  'prized',\n",
       "  'location',\n",
       "  'on',\n",
       "  'Michigan_Avenue',\n",
       "  'While',\n",
       "  'this',\n",
       "  'address',\n",
       "  'in',\n",
       "  'Chicago',\n",
       "  'requires',\n",
       "  'a',\n",
       "  'high_level_of_quality',\n",
       "  'the',\n",
       "  'Omni',\n",
       "  'delivers',\n",
       "  'Check',\n",
       "  'in',\n",
       "  'for',\n",
       "  'myself',\n",
       "  'and',\n",
       "  'a',\n",
       "  'whole_group',\n",
       "  'of',\n",
       "  'people',\n",
       "  'with',\n",
       "  'me',\n",
       "  'was',\n",
       "  'under',\n",
       "  'minutes',\n",
       "  'the',\n",
       "  'staff',\n",
       "  'had',\n",
       "  'plentiful',\n",
       "  'recommendations_for_dining',\n",
       "  'and',\n",
       "  'events',\n",
       "  'and',\n",
       "  'the',\n",
       "  'rooms',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'you',\n",
       "  'll_find',\n",
       "  'at',\n",
       "  'this',\n",
       "  'price_range',\n",
       "  'in',\n",
       "  'Chicago',\n",
       "  'Even',\n",
       "  'the',\n",
       "  'standard_room',\n",
       "  'has',\n",
       "  'a',\n",
       "  'separate_living_area',\n",
       "  'and',\n",
       "  'work_desk',\n",
       "  'The',\n",
       "  'fitness_center',\n",
       "  'has',\n",
       "  'free_weights',\n",
       "  'weight_machines',\n",
       "  'and',\n",
       "  'two',\n",
       "  'rows',\n",
       "  'of',\n",
       "  'cardio_equipment',\n",
       "  'I',\n",
       "  'shared',\n",
       "  'the',\n",
       "  'room',\n",
       "  'with',\n",
       "  'others',\n",
       "  'and',\n",
       "  'did',\n",
       "  'not',\n",
       "  'feel_cramped',\n",
       "  'in',\n",
       "  'any',\n",
       "  'way',\n",
       "  'All',\n",
       "  'in',\n",
       "  'all',\n",
       "  'a',\n",
       "  'great',\n",
       "  'property'],\n",
       " ['I',\n",
       "  'asked_for_a_high_floor',\n",
       "  'away_from_the_elevator',\n",
       "  'and',\n",
       "  'that',\n",
       "  'is',\n",
       "  'what',\n",
       "  'I',\n",
       "  'got',\n",
       "  'The',\n",
       "  'room',\n",
       "  'was',\n",
       "  'pleasantly_decorated',\n",
       "  'functional',\n",
       "  'and',\n",
       "  'very',\n",
       "  'clean',\n",
       "  'I',\n",
       "  'didn_t_need',\n",
       "  'a',\n",
       "  'whole_lot',\n",
       "  'of',\n",
       "  'service',\n",
       "  'but',\n",
       "  'when',\n",
       "  'I',\n",
       "  'did',\n",
       "  'they',\n",
       "  'were',\n",
       "  'pleasant',\n",
       "  'and',\n",
       "  'prompt',\n",
       "  'I',\n",
       "  'used_the_fitness_center',\n",
       "  'which',\n",
       "  'was',\n",
       "  'well_equipped',\n",
       "  'and',\n",
       "  'everything_was_in_working_order',\n",
       "  'It',\n",
       "  'is',\n",
       "  'in',\n",
       "  'a',\n",
       "  'great_location',\n",
       "  'at',\n",
       "  'one_end',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Michigan_Avenue',\n",
       "  'shopping_district'],\n",
       " ['I',\n",
       "  'stayed_at_the_Omni',\n",
       "  'for',\n",
       "  'one_night',\n",
       "  'following',\n",
       "  'a',\n",
       "  'business_meeting',\n",
       "  'at',\n",
       "  'another',\n",
       "  'downtown_Chicago',\n",
       "  'hotel',\n",
       "  'I',\n",
       "  'was',\n",
       "  'completely',\n",
       "  'impressed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'service',\n",
       "  'all',\n",
       "  'personnel',\n",
       "  'during',\n",
       "  'my',\n",
       "  'stay',\n",
       "  'were',\n",
       "  'absolutely_outstanding',\n",
       "  'I',\n",
       "  'checked',\n",
       "  'in',\n",
       "  'quite_early',\n",
       "  'no',\n",
       "  'problem',\n",
       "  'and',\n",
       "  'was',\n",
       "  'efficiently',\n",
       "  'checked',\n",
       "  'in',\n",
       "  'My',\n",
       "  'room',\n",
       "  'had',\n",
       "  'a',\n",
       "  'somewhat',\n",
       "  'heavy_scent',\n",
       "  'of',\n",
       "  'air_freshener',\n",
       "  'the',\n",
       "  'ONLY_negative',\n",
       "  'from',\n",
       "  'the',\n",
       "  'entire_stay',\n",
       "  'which',\n",
       "  'was',\n",
       "  'managed',\n",
       "  'reasonably_well',\n",
       "  'by',\n",
       "  'opening_the_windows',\n",
       "  'I',\n",
       "  'don_t',\n",
       "  'generally',\n",
       "  'require',\n",
       "  'much',\n",
       "  'during',\n",
       "  'my',\n",
       "  'hotel_stays',\n",
       "  'but',\n",
       "  'suffice',\n",
       "  'to',\n",
       "  'say',\n",
       "  'the',\n",
       "  'doorman',\n",
       "  'housekeeping',\n",
       "  'the',\n",
       "  'night_manager',\n",
       "  'and',\n",
       "  'bartender',\n",
       "  'at',\n",
       "  'the',\n",
       "  'day',\n",
       "  'waiter',\n",
       "  'at',\n",
       "  'and',\n",
       "  'the',\n",
       "  'concierge',\n",
       "  'were',\n",
       "  'amazing',\n",
       "  'I',\n",
       "  'never_waited',\n",
       "  'more',\n",
       "  'than',\n",
       "  'about',\n",
       "  'seconds',\n",
       "  'for',\n",
       "  'anything',\n",
       "  'The',\n",
       "  'room',\n",
       "  'was',\n",
       "  'very',\n",
       "  'comfy',\n",
       "  'and',\n",
       "  'the',\n",
       "  'amenities',\n",
       "  'were',\n",
       "  'superior',\n",
       "  'One',\n",
       "  'very',\n",
       "  'tiny_complaint',\n",
       "  'there',\n",
       "  'was',\n",
       "  'no',\n",
       "  'wastebasket',\n",
       "  'near_the_sink',\n",
       "  'or',\n",
       "  'near',\n",
       "  'the',\n",
       "  'wet',\n",
       "  'bar',\n",
       "  'had',\n",
       "  'to',\n",
       "  'walk',\n",
       "  'to',\n",
       "  'the',\n",
       "  'other',\n",
       "  'end',\n",
       "  'of',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  'or',\n",
       "  'sitting_room',\n",
       "  'to',\n",
       "  'dispose',\n",
       "  'of',\n",
       "  'kleenex',\n",
       "  'coffee',\n",
       "  'paraphernalia',\n",
       "  'One',\n",
       "  'wastebasket',\n",
       "  'would',\n",
       "  'make_all_the_difference',\n",
       "  'All',\n",
       "  'that',\n",
       "  'said',\n",
       "  'what',\n",
       "  'a',\n",
       "  'great',\n",
       "  'hotel',\n",
       "  'Thanks',\n",
       "  'Omni',\n",
       "  'I',\n",
       "  'had',\n",
       "  'a',\n",
       "  'great_stay']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_corpus[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unlabelled_corpus, open('./input/unlabelled_corpus_clean1.p','wb'))\n",
    "pickle.dump(fake_corpus, open('./input/fake_corpus_clean1.p','wb'))\n",
    "pickle.dump(true_corpus, open('./input/true_corpus_clean1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stream(df, stopword_set):\n",
    "    sentence_stream = []\n",
    "    for s in df:\n",
    "        s2 = [c  for c in s if c not in stopword_set] \n",
    "        sentence_stream.append(s2)\n",
    "    return sentence_stream\n",
    "\n",
    "unlabelled_corpus = filter_stream(unlabelled_corpus, all_stop_set)\n",
    "true_corpus = filter_stream(true_corpus, all_stop_set)\n",
    "fake_corpus = filter_stream(fake_corpus, all_stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unlabelled_corpus, open('./input/unlabelled_corpus_clean2.p','wb'))\n",
    "pickle.dump(fake_corpus, open('./input/fake_corpus_clean2.p','wb'))\n",
    "pickle.dump(true_corpus, open('./input/true_corpus_clean2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
