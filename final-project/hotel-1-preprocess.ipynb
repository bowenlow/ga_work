{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import gensim,logging\n",
    "from gensim.parsing import PorterStemmer\n",
    "from gensim.models import Word2Vec, Doc2Vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import nltk\n",
    "import pickle\n",
    "import warnings\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "np.random.seed(0)\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/deceptive-opinion.csv')\n",
    "\n",
    "stop_words = pd.read_csv('./input/stopwords.csv',names=['stop'])\n",
    "new_stop = stop_words.stop.map(lambda x: str.capitalize(x))\n",
    "all_stop_set = set(stop_words.stop.append(new_stop,ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./input/Hotel_Reviews.csv')\n",
    "data2 = df2[~df2['lat'].isnull()]\n",
    "data2 = data2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unlabelled = []\n",
    "for i,line in enumerate(data2['Negative_Review']):\n",
    "    inp = \"\"\n",
    "    if (np.random.randint(0,2) == 0):\n",
    "        inp = data2.loc[i,'Negative_Review'] + ' ' + data2.loc[i,'Positive_Review']\n",
    "    else:\n",
    "        inp = data2.loc[i,'Positive_Review'] + ' ' + data2.loc[i,'Negative_Review']\n",
    "    all_unlabelled.append(inp)\n",
    "\n",
    "data2['All_Text'] = pd.Series(all_unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All_Text</th>\n",
       "      <th>Negative_Review</th>\n",
       "      <th>Positive_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am so angry that i made this post available...</td>\n",
       "      <td>I am so angry that i made this post available...</td>\n",
       "      <td>Only the park outside of the hotel was beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No real complaints the hotel was great great ...</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>No real complaints the hotel was great great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Location was good and staff were ok It is cut...</td>\n",
       "      <td>Rooms are nice but for elderly a bit difficul...</td>\n",
       "      <td>Location was good and staff were ok It is cut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            All_Text  \\\n",
       "0   I am so angry that i made this post available...   \n",
       "1   No real complaints the hotel was great great ...   \n",
       "2   Location was good and staff were ok It is cut...   \n",
       "\n",
       "                                     Negative_Review  \\\n",
       "0   I am so angry that i made this post available...   \n",
       "1                                        No Negative   \n",
       "2   Rooms are nice but for elderly a bit difficul...   \n",
       "\n",
       "                                     Positive_Review  \n",
       "0   Only the park outside of the hotel was beauti...  \n",
       "1   No real complaints the hotel was great great ...  \n",
       "2   Location was good and staff were ok It is cut...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[['All_Text','Negative_Review','Positive_Review']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate combinations of all_stop_words, bigrams\n",
    "#generate bigram\n",
    "bigram_stops=[]\n",
    "for a in all_stop_set:\n",
    "    for b in all_stop_set:\n",
    "        bigram_stop = a+\"_\"+b\n",
    "        bigram_stops.append(bigram_stop)\n",
    "all_stop_set = all_stop_set.union(bigram_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate truth and fake df\n",
    "truedf = df[df.deceptive=='truthful'].loc[:,'text']\n",
    "fakedf = df[df.deceptive=='deceptive'].loc[:,'text']\n",
    "truedfy = df[df.deceptive=='truthful'].loc[:,'deceptive']\n",
    "fakedfy = df[df.deceptive=='deceptive'].loc[:,'deceptive']\n",
    "truedfy.replace({'truthful':1},inplace=True)\n",
    "fakedfy.replace({'deceptive':0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(sentdf, tokens_only=False):\n",
    "    for i, line in enumerate(sentdf):\n",
    "        if tokens_only:\n",
    "            yield list(gensim.utils.tokenize(line))\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(list(gensim.utils.tokenize(line)),[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# def to_list(doc):\n",
    "#     return [t.text for t in doc]\n",
    "\n",
    "# true_corpus = [to_list(nlp(s))[:-1] for ind,s in enumerate(truedf)]\n",
    "# fake_corpus = [to_list(nlp(s))[:-1] for ind,s in enumerate(fakedf)]\n",
    "\n",
    "# temp_arr = []\n",
    "# for ind,s in enumerate(data2['All_Text']):\n",
    "#     if ind % 100 == 0:\n",
    "#         print (ind)\n",
    "#     temp_arr.append(to_list(nlp(s))[:-1])\n",
    "# unlabelled_corpus = temp_arr.copy()\n",
    "true_corpus = list(read_corpus(truedf,tokens_only=True))\n",
    "fake_corpus = list(read_corpus(fakedf,tokens_only=True))\n",
    "unlabelled_corpus = list(read_corpus(data2['All_Text'], tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 10:12:58,563 : INFO : collecting all words and their counts\n",
      "2018-01-25 10:12:58,564 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-25 10:12:59,111 : INFO : PROGRESS: at sentence #10000, processed 381532 words and 134268 word types\n",
      "2018-01-25 10:12:59,568 : INFO : PROGRESS: at sentence #20000, processed 691920 words and 203976 word types\n",
      "2018-01-25 10:13:00,047 : INFO : PROGRESS: at sentence #30000, processed 1023549 words and 269582 word types\n",
      "2018-01-25 10:13:00,523 : INFO : PROGRESS: at sentence #40000, processed 1357244 words and 328018 word types\n",
      "2018-01-25 10:13:01,098 : INFO : PROGRESS: at sentence #50000, processed 1696278 words and 381982 word types\n",
      "2018-01-25 10:13:01,588 : INFO : PROGRESS: at sentence #60000, processed 2018082 words and 431703 word types\n",
      "2018-01-25 10:13:02,079 : INFO : PROGRESS: at sentence #70000, processed 2359057 words and 483652 word types\n",
      "2018-01-25 10:13:02,561 : INFO : PROGRESS: at sentence #80000, processed 2694422 words and 528984 word types\n",
      "2018-01-25 10:13:03,098 : INFO : PROGRESS: at sentence #90000, processed 3018990 words and 570999 word types\n",
      "2018-01-25 10:13:03,719 : INFO : PROGRESS: at sentence #100000, processed 3360463 words and 615510 word types\n",
      "2018-01-25 10:13:04,345 : INFO : PROGRESS: at sentence #110000, processed 3693480 words and 658586 word types\n",
      "2018-01-25 10:13:04,909 : INFO : PROGRESS: at sentence #120000, processed 4044074 words and 701700 word types\n",
      "2018-01-25 10:13:05,412 : INFO : PROGRESS: at sentence #130000, processed 4389099 words and 739740 word types\n",
      "2018-01-25 10:13:05,911 : INFO : PROGRESS: at sentence #140000, processed 4742291 words and 779999 word types\n",
      "2018-01-25 10:13:06,399 : INFO : PROGRESS: at sentence #150000, processed 5073711 words and 814304 word types\n",
      "2018-01-25 10:13:06,858 : INFO : PROGRESS: at sentence #160000, processed 5400404 words and 848008 word types\n",
      "2018-01-25 10:13:07,311 : INFO : PROGRESS: at sentence #170000, processed 5723375 words and 880896 word types\n",
      "2018-01-25 10:13:07,766 : INFO : PROGRESS: at sentence #180000, processed 6048936 words and 912821 word types\n",
      "2018-01-25 10:13:08,232 : INFO : PROGRESS: at sentence #190000, processed 6392150 words and 947534 word types\n",
      "2018-01-25 10:13:08,700 : INFO : PROGRESS: at sentence #200000, processed 6733267 words and 980553 word types\n",
      "2018-01-25 10:13:09,185 : INFO : PROGRESS: at sentence #210000, processed 7074442 words and 1013826 word types\n",
      "2018-01-25 10:13:09,682 : INFO : PROGRESS: at sentence #220000, processed 7415330 words and 1045736 word types\n",
      "2018-01-25 10:13:10,138 : INFO : PROGRESS: at sentence #230000, processed 7743819 words and 1076150 word types\n",
      "2018-01-25 10:13:10,610 : INFO : PROGRESS: at sentence #240000, processed 8079151 words and 1105328 word types\n",
      "2018-01-25 10:13:11,095 : INFO : PROGRESS: at sentence #250000, processed 8419066 words and 1135996 word types\n",
      "2018-01-25 10:13:11,708 : INFO : PROGRESS: at sentence #260000, processed 8761160 words and 1165414 word types\n",
      "2018-01-25 10:13:12,266 : INFO : PROGRESS: at sentence #270000, processed 9099380 words and 1195759 word types\n",
      "2018-01-25 10:13:12,912 : INFO : PROGRESS: at sentence #280000, processed 9466883 words and 1230805 word types\n",
      "2018-01-25 10:13:13,588 : INFO : PROGRESS: at sentence #290000, processed 9822096 words and 1262489 word types\n",
      "2018-01-25 10:13:14,092 : INFO : PROGRESS: at sentence #300000, processed 10141722 words and 1290974 word types\n",
      "2018-01-25 10:13:14,587 : INFO : PROGRESS: at sentence #310000, processed 10480892 words and 1321188 word types\n",
      "2018-01-25 10:13:15,101 : INFO : PROGRESS: at sentence #320000, processed 10827476 words and 1351571 word types\n",
      "2018-01-25 10:13:15,600 : INFO : PROGRESS: at sentence #330000, processed 11167259 words and 1381360 word types\n",
      "2018-01-25 10:13:16,169 : INFO : PROGRESS: at sentence #340000, processed 11504399 words and 1410015 word types\n",
      "2018-01-25 10:13:16,633 : INFO : PROGRESS: at sentence #350000, processed 11841612 words and 1437979 word types\n",
      "2018-01-25 10:13:17,109 : INFO : PROGRESS: at sentence #360000, processed 12185088 words and 1466680 word types\n",
      "2018-01-25 10:13:17,571 : INFO : PROGRESS: at sentence #370000, processed 12516982 words and 1494216 word types\n",
      "2018-01-25 10:13:18,057 : INFO : PROGRESS: at sentence #380000, processed 12864815 words and 1523039 word types\n",
      "2018-01-25 10:13:18,521 : INFO : PROGRESS: at sentence #390000, processed 13201725 words and 1549315 word types\n",
      "2018-01-25 10:13:18,997 : INFO : PROGRESS: at sentence #400000, processed 13539384 words and 1576980 word types\n",
      "2018-01-25 10:13:19,473 : INFO : PROGRESS: at sentence #410000, processed 13878075 words and 1605035 word types\n",
      "2018-01-25 10:13:19,992 : INFO : PROGRESS: at sentence #420000, processed 14241877 words and 1634936 word types\n",
      "2018-01-25 10:13:20,485 : INFO : PROGRESS: at sentence #430000, processed 14600630 words and 1662222 word types\n",
      "2018-01-25 10:13:21,000 : INFO : PROGRESS: at sentence #440000, processed 14960106 words and 1688628 word types\n",
      "2018-01-25 10:13:21,421 : INFO : PROGRESS: at sentence #450000, processed 15271394 words and 1712822 word types\n",
      "2018-01-25 10:13:21,923 : INFO : PROGRESS: at sentence #460000, processed 15622687 words and 1737620 word types\n",
      "2018-01-25 10:13:22,404 : INFO : PROGRESS: at sentence #470000, processed 15950723 words and 1761627 word types\n",
      "2018-01-25 10:13:22,875 : INFO : PROGRESS: at sentence #480000, processed 16291950 words and 1789231 word types\n",
      "2018-01-25 10:13:23,353 : INFO : PROGRESS: at sentence #490000, processed 16630040 words and 1815448 word types\n",
      "2018-01-25 10:13:23,849 : INFO : PROGRESS: at sentence #500000, processed 16983304 words and 1841895 word types\n",
      "2018-01-25 10:13:24,373 : INFO : PROGRESS: at sentence #510000, processed 17349557 words and 1868656 word types\n",
      "2018-01-25 10:13:24,812 : INFO : collected 1896161 word types from a corpus of 17662986 words (unigram + bigrams) and 514070 sentences\n",
      "2018-01-25 10:13:24,812 : INFO : using 1896161 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-01-25 10:13:24,876 : INFO : source_vocab length 1896161\n",
      "2018-01-25 10:13:40,482 : INFO : Phraser built with 12000 12000 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "sentence_stream = [s for s in unlabelled_corpus + true_corpus + fake_corpus]\n",
    "phrases = Phrases(sentence_stream)\n",
    "bigram = Phraser(phrases)\n",
    "sentence_stream = [bigram[s] for s in sentence_stream]\n",
    "unlabelled_corpus = [bigram[s] for s in unlabelled_corpus]\n",
    "true_corpus = [bigram[s] for s in true_corpus]\n",
    "fake_corpus = [bigram[s] for s in fake_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 10:17:00,138 : INFO : collecting all words and their counts\n",
      "2018-01-25 10:17:00,138 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-25 10:17:00,632 : INFO : PROGRESS: at sentence #10000, processed 361516 words and 143367 word types\n",
      "2018-01-25 10:17:01,035 : INFO : PROGRESS: at sentence #20000, processed 655926 words and 220404 word types\n",
      "2018-01-25 10:17:01,496 : INFO : PROGRESS: at sentence #30000, processed 969993 words and 294000 word types\n",
      "2018-01-25 10:17:01,936 : INFO : PROGRESS: at sentence #40000, processed 1285848 words and 360255 word types\n",
      "2018-01-25 10:17:02,388 : INFO : PROGRESS: at sentence #50000, processed 1606653 words and 421960 word types\n",
      "2018-01-25 10:17:02,797 : INFO : PROGRESS: at sentence #60000, processed 1911086 words and 478690 word types\n",
      "2018-01-25 10:17:03,253 : INFO : PROGRESS: at sentence #70000, processed 2234079 words and 537783 word types\n",
      "2018-01-25 10:17:03,693 : INFO : PROGRESS: at sentence #80000, processed 2551605 words and 590085 word types\n",
      "2018-01-25 10:17:04,142 : INFO : PROGRESS: at sentence #90000, processed 2859242 words and 638367 word types\n",
      "2018-01-25 10:17:04,595 : INFO : PROGRESS: at sentence #100000, processed 3182790 words and 689606 word types\n",
      "2018-01-25 10:17:05,068 : INFO : PROGRESS: at sentence #110000, processed 3498191 words and 739219 word types\n",
      "2018-01-25 10:17:05,533 : INFO : PROGRESS: at sentence #120000, processed 3829704 words and 789323 word types\n",
      "2018-01-25 10:17:05,989 : INFO : PROGRESS: at sentence #130000, processed 4156595 words and 833612 word types\n",
      "2018-01-25 10:17:06,456 : INFO : PROGRESS: at sentence #140000, processed 4491430 words and 880355 word types\n",
      "2018-01-25 10:17:06,893 : INFO : PROGRESS: at sentence #150000, processed 4805323 words and 920480 word types\n",
      "2018-01-25 10:17:07,328 : INFO : PROGRESS: at sentence #160000, processed 5115336 words and 959614 word types\n",
      "2018-01-25 10:17:07,745 : INFO : PROGRESS: at sentence #170000, processed 5421554 words and 998145 word types\n",
      "2018-01-25 10:17:08,185 : INFO : PROGRESS: at sentence #180000, processed 5730000 words and 1035375 word types\n",
      "2018-01-25 10:17:08,646 : INFO : PROGRESS: at sentence #190000, processed 6055081 words and 1076116 word types\n",
      "2018-01-25 10:17:09,095 : INFO : PROGRESS: at sentence #200000, processed 6378401 words and 1114705 word types\n",
      "2018-01-25 10:17:09,545 : INFO : PROGRESS: at sentence #210000, processed 6701859 words and 1153559 word types\n",
      "2018-01-25 10:17:10,080 : INFO : PROGRESS: at sentence #220000, processed 7024954 words and 1191094 word types\n",
      "2018-01-25 10:17:10,583 : INFO : PROGRESS: at sentence #230000, processed 7335662 words and 1227011 word types\n",
      "2018-01-25 10:17:11,048 : INFO : PROGRESS: at sentence #240000, processed 7652657 words and 1261801 word types\n",
      "2018-01-25 10:17:11,548 : INFO : PROGRESS: at sentence #250000, processed 7975304 words and 1297887 word types\n",
      "2018-01-25 10:17:12,111 : INFO : PROGRESS: at sentence #260000, processed 8299465 words and 1332850 word types\n",
      "2018-01-25 10:17:12,670 : INFO : PROGRESS: at sentence #270000, processed 8620075 words and 1368605 word types\n",
      "2018-01-25 10:17:13,322 : INFO : PROGRESS: at sentence #280000, processed 8968233 words and 1409964 word types\n",
      "2018-01-25 10:17:13,893 : INFO : PROGRESS: at sentence #290000, processed 9304721 words and 1447563 word types\n",
      "2018-01-25 10:17:14,411 : INFO : PROGRESS: at sentence #300000, processed 9607581 words and 1481104 word types\n",
      "2018-01-25 10:17:14,923 : INFO : PROGRESS: at sentence #310000, processed 9929727 words and 1516702 word types\n",
      "2018-01-25 10:17:15,421 : INFO : PROGRESS: at sentence #320000, processed 10258018 words and 1552644 word types\n",
      "2018-01-25 10:17:15,942 : INFO : PROGRESS: at sentence #330000, processed 10579758 words and 1587920 word types\n",
      "2018-01-25 10:17:16,469 : INFO : PROGRESS: at sentence #340000, processed 10899662 words and 1621694 word types\n",
      "2018-01-25 10:17:16,968 : INFO : PROGRESS: at sentence #350000, processed 11220172 words and 1654629 word types\n",
      "2018-01-25 10:17:17,478 : INFO : PROGRESS: at sentence #360000, processed 11546511 words and 1688491 word types\n",
      "2018-01-25 10:17:17,981 : INFO : PROGRESS: at sentence #370000, processed 11861252 words and 1720990 word types\n",
      "2018-01-25 10:17:18,435 : INFO : PROGRESS: at sentence #380000, processed 12191270 words and 1755008 word types\n",
      "2018-01-25 10:17:18,889 : INFO : PROGRESS: at sentence #390000, processed 12511142 words and 1786042 word types\n",
      "2018-01-25 10:17:19,357 : INFO : PROGRESS: at sentence #400000, processed 12831985 words and 1818326 word types\n",
      "2018-01-25 10:17:19,877 : INFO : PROGRESS: at sentence #410000, processed 13153917 words and 1851275 word types\n",
      "2018-01-25 10:17:20,446 : INFO : PROGRESS: at sentence #420000, processed 13499467 words and 1886477 word types\n",
      "2018-01-25 10:17:20,953 : INFO : PROGRESS: at sentence #430000, processed 13839733 words and 1918911 word types\n",
      "2018-01-25 10:17:21,479 : INFO : PROGRESS: at sentence #440000, processed 14180516 words and 1950590 word types\n",
      "2018-01-25 10:17:21,890 : INFO : PROGRESS: at sentence #450000, processed 14476417 words and 1979006 word types\n",
      "2018-01-25 10:17:22,357 : INFO : PROGRESS: at sentence #460000, processed 14809361 words and 2008500 word types\n",
      "2018-01-25 10:17:22,796 : INFO : PROGRESS: at sentence #470000, processed 15120759 words and 2036879 word types\n",
      "2018-01-25 10:17:23,263 : INFO : PROGRESS: at sentence #480000, processed 15445052 words and 2069195 word types\n",
      "2018-01-25 10:17:23,722 : INFO : PROGRESS: at sentence #490000, processed 15766771 words and 2099925 word types\n",
      "2018-01-25 10:17:24,201 : INFO : PROGRESS: at sentence #500000, processed 16102443 words and 2131071 word types\n",
      "2018-01-25 10:17:24,686 : INFO : PROGRESS: at sentence #510000, processed 16449817 words and 2162830 word types\n",
      "2018-01-25 10:17:25,100 : INFO : collected 2194936 word types from a corpus of 16747036 words (unigram + bigrams) and 514070 sentences\n",
      "2018-01-25 10:17:25,116 : INFO : using 2194936 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-01-25 10:17:25,174 : INFO : source_vocab length 2194936\n",
      "2018-01-25 10:17:42,169 : INFO : Phraser added 50000 phrasegrams\n",
      "2018-01-25 10:17:49,435 : INFO : Phraser built with 50586 50586 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sentence_stream)\n",
    "trigram = Phraser(phrases)\n",
    "sentence_stream = [trigram[s] for s in sentence_stream]\n",
    "unlabelled_corpus = [trigram[s] for s in unlabelled_corpus]\n",
    "true_corpus = [trigram[s] for s in true_corpus]\n",
    "fake_corpus = [trigram[s] for s in fake_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We',\n",
       "  'stayed',\n",
       "  'for',\n",
       "  'a',\n",
       "  'one',\n",
       "  'night',\n",
       "  'getaway',\n",
       "  'with',\n",
       "  'family',\n",
       "  'on',\n",
       "  'a',\n",
       "  'thursday',\n",
       "  'Triple',\n",
       "  'AAA',\n",
       "  'rate',\n",
       "  'of',\n",
       "  'was',\n",
       "  'a',\n",
       "  'steal',\n",
       "  'th_floor',\n",
       "  'room',\n",
       "  'complete',\n",
       "  'with',\n",
       "  'in',\n",
       "  'plasma_TV',\n",
       "  'bose',\n",
       "  'stereo',\n",
       "  'voss',\n",
       "  'and',\n",
       "  'evian',\n",
       "  'water',\n",
       "  'and',\n",
       "  'gorgeous',\n",
       "  'bathroom',\n",
       "  'no',\n",
       "  'tub',\n",
       "  'but',\n",
       "  'was',\n",
       "  'fine',\n",
       "  'for',\n",
       "  'us',\n",
       "  'Concierge',\n",
       "  'was',\n",
       "  'very',\n",
       "  'helpful',\n",
       "  'You_cannot_beat',\n",
       "  'this',\n",
       "  'location',\n",
       "  'Only',\n",
       "  'flaw',\n",
       "  'was',\n",
       "  'breakfast',\n",
       "  'was',\n",
       "  'pricey',\n",
       "  'and',\n",
       "  'service',\n",
       "  'was',\n",
       "  'very',\n",
       "  'very',\n",
       "  'slow',\n",
       "  'hours',\n",
       "  'for',\n",
       "  'four',\n",
       "  'kids',\n",
       "  'and',\n",
       "  'four_adults',\n",
       "  'on',\n",
       "  'a',\n",
       "  'friday',\n",
       "  'morning',\n",
       "  'even_though',\n",
       "  'there',\n",
       "  'were',\n",
       "  'only',\n",
       "  'two',\n",
       "  'other',\n",
       "  'tables',\n",
       "  'in',\n",
       "  'the',\n",
       "  'restaurant',\n",
       "  'Food',\n",
       "  'was',\n",
       "  'very',\n",
       "  'good',\n",
       "  'so',\n",
       "  'it',\n",
       "  'was',\n",
       "  'worth',\n",
       "  'the',\n",
       "  'wait',\n",
       "  'I',\n",
       "  'would',\n",
       "  'return',\n",
       "  'in',\n",
       "  'a',\n",
       "  'heartbeat',\n",
       "  'A',\n",
       "  'gem',\n",
       "  'in',\n",
       "  'chicago'],\n",
       " ['Triple',\n",
       "  'A',\n",
       "  'rate',\n",
       "  'with',\n",
       "  'upgrade',\n",
       "  'to',\n",
       "  'view',\n",
       "  'room',\n",
       "  'was',\n",
       "  'less_than',\n",
       "  'which',\n",
       "  'also',\n",
       "  'included',\n",
       "  'breakfast',\n",
       "  'vouchers',\n",
       "  'Had',\n",
       "  'a',\n",
       "  'great',\n",
       "  'view',\n",
       "  'of',\n",
       "  'river',\n",
       "  'lake',\n",
       "  'Wrigley',\n",
       "  'Bldg',\n",
       "  'Tribune',\n",
       "  'Bldg',\n",
       "  'Most',\n",
       "  'major',\n",
       "  'restaurants',\n",
       "  'Shopping',\n",
       "  'Sightseeing',\n",
       "  'attractions',\n",
       "  'within_walking_distance',\n",
       "  'Large',\n",
       "  'room',\n",
       "  'with',\n",
       "  'a',\n",
       "  'very',\n",
       "  'comfortable',\n",
       "  'bed'],\n",
       " ['This',\n",
       "  'comes',\n",
       "  'a',\n",
       "  'little',\n",
       "  'late',\n",
       "  'as',\n",
       "  'I',\n",
       "  'm',\n",
       "  'finally',\n",
       "  'catching',\n",
       "  'up',\n",
       "  'on',\n",
       "  'my',\n",
       "  'reviews',\n",
       "  'from',\n",
       "  'the',\n",
       "  'past',\n",
       "  'several_months',\n",
       "  'A',\n",
       "  'dear',\n",
       "  'friend',\n",
       "  'and',\n",
       "  'I',\n",
       "  'stayed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Hyatt_Regency',\n",
       "  'in',\n",
       "  'late_October',\n",
       "  'for',\n",
       "  'one',\n",
       "  'night',\n",
       "  'while',\n",
       "  'visiting',\n",
       "  'a',\n",
       "  'friend',\n",
       "  'and',\n",
       "  'her_husband',\n",
       "  'from',\n",
       "  'out',\n",
       "  'of',\n",
       "  'town',\n",
       "  'This',\n",
       "  'hotel',\n",
       "  'is',\n",
       "  'perfect',\n",
       "  'IMO',\n",
       "  'Easy_check',\n",
       "  'in',\n",
       "  'and',\n",
       "  'check_out',\n",
       "  'Lovely',\n",
       "  'clean',\n",
       "  'comfortable',\n",
       "  'rooms',\n",
       "  'with',\n",
       "  'great',\n",
       "  'views',\n",
       "  'of',\n",
       "  'the',\n",
       "  'city',\n",
       "  'I',\n",
       "  'know',\n",
       "  'this',\n",
       "  'area',\n",
       "  'pretty',\n",
       "  'well',\n",
       "  'and',\n",
       "  'it',\n",
       "  's',\n",
       "  'very',\n",
       "  'convenient',\n",
       "  'to',\n",
       "  'many',\n",
       "  'downtown_Chicago',\n",
       "  'attractions',\n",
       "  'We',\n",
       "  'had',\n",
       "  'dinner',\n",
       "  'and',\n",
       "  'went',\n",
       "  'clubing',\n",
       "  'with',\n",
       "  'our',\n",
       "  'friends',\n",
       "  'around',\n",
       "  'Division',\n",
       "  'St',\n",
       "  'We',\n",
       "  'had',\n",
       "  'no',\n",
       "  'problems',\n",
       "  'getting_cabs',\n",
       "  'back',\n",
       "  'and',\n",
       "  'forth',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Hyatt',\n",
       "  'and',\n",
       "  'there',\n",
       "  's',\n",
       "  'even',\n",
       "  'public_transportation',\n",
       "  'right',\n",
       "  'near',\n",
       "  'by',\n",
       "  'but',\n",
       "  'we',\n",
       "  'didn_t_bother',\n",
       "  'since',\n",
       "  'we',\n",
       "  'only',\n",
       "  'needed',\n",
       "  'cabs',\n",
       "  'from',\n",
       "  'and',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hotel',\n",
       "  'Parking',\n",
       "  'as',\n",
       "  'is',\n",
       "  'usual',\n",
       "  'for',\n",
       "  'Chicago',\n",
       "  'was',\n",
       "  'expensive',\n",
       "  'but',\n",
       "  'we',\n",
       "  'were',\n",
       "  'able',\n",
       "  'to',\n",
       "  'get',\n",
       "  'our',\n",
       "  'car',\n",
       "  'out',\n",
       "  'quickly',\n",
       "  'however',\n",
       "  'we',\n",
       "  'left',\n",
       "  'on',\n",
       "  'a',\n",
       "  'Sunday_morning',\n",
       "  'not',\n",
       "  'exactly',\n",
       "  'a',\n",
       "  'high',\n",
       "  'traffic',\n",
       "  'time',\n",
       "  'although',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'Bears',\n",
       "  'homegame',\n",
       "  'day',\n",
       "  'so',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'busier',\n",
       "  'than_usual',\n",
       "  'I',\n",
       "  'would',\n",
       "  'think',\n",
       "  'No',\n",
       "  'problems',\n",
       "  'at',\n",
       "  'all',\n",
       "  'and',\n",
       "  'the',\n",
       "  'best_part',\n",
       "  'is',\n",
       "  'that',\n",
       "  'we',\n",
       "  'got',\n",
       "  'a',\n",
       "  'rate',\n",
       "  'of',\n",
       "  'through',\n",
       "  'Hotwire',\n",
       "  'a',\n",
       "  'downright',\n",
       "  'steal',\n",
       "  'for',\n",
       "  'this',\n",
       "  'area',\n",
       "  'of',\n",
       "  'Chicago',\n",
       "  'and',\n",
       "  'the',\n",
       "  'quality',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hotel']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unlabelled_corpus, open('./input/unlabelled_corpus_raw.p','wb'))\n",
    "pickle.dump(fake_corpus, open('./input/fake_corpus_raw.p','wb'))\n",
    "pickle.dump(true_corpus, open('./input/true_corpus_raw.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stayed',\n",
       "  'one_night',\n",
       "  'getaway',\n",
       "  'family',\n",
       "  'thursday',\n",
       "  'Triple',\n",
       "  'AAA',\n",
       "  'rate',\n",
       "  'steal',\n",
       "  'th_floor',\n",
       "  'room',\n",
       "  'complete',\n",
       "  'plasma_TV',\n",
       "  'bose',\n",
       "  'stereo',\n",
       "  'voss',\n",
       "  'evian',\n",
       "  'water',\n",
       "  'gorgeous',\n",
       "  'bathroom',\n",
       "  'tub',\n",
       "  'fine',\n",
       "  'us',\n",
       "  'Concierge',\n",
       "  'helpful',\n",
       "  'beat',\n",
       "  'location',\n",
       "  'flaw',\n",
       "  'breakfast',\n",
       "  'pricey',\n",
       "  'service',\n",
       "  'slow',\n",
       "  'hours',\n",
       "  'four',\n",
       "  'kids',\n",
       "  'four_adults',\n",
       "  'friday',\n",
       "  'morning',\n",
       "  'even_though',\n",
       "  'two',\n",
       "  'tables',\n",
       "  'restaurant',\n",
       "  'Food',\n",
       "  'good',\n",
       "  'worth',\n",
       "  'wait_return',\n",
       "  'heartbeat',\n",
       "  'gem',\n",
       "  'chicago'],\n",
       " ['Triple',\n",
       "  'rate',\n",
       "  'upgrade',\n",
       "  'view',\n",
       "  'room',\n",
       "  'less_than',\n",
       "  'also',\n",
       "  'included',\n",
       "  'breakfast',\n",
       "  'vouchers',\n",
       "  'great',\n",
       "  'view_river',\n",
       "  'lake',\n",
       "  'Wrigley',\n",
       "  'Bldg',\n",
       "  'Tribune',\n",
       "  'Bldg',\n",
       "  'major',\n",
       "  'restaurants',\n",
       "  'Shopping',\n",
       "  'Sightseeing',\n",
       "  'attractions',\n",
       "  'within_walking_distance',\n",
       "  'Large',\n",
       "  'room',\n",
       "  'comfortable',\n",
       "  'bed'],\n",
       " ['comes',\n",
       "  'little',\n",
       "  'late',\n",
       "  'm',\n",
       "  'finally',\n",
       "  'catching',\n",
       "  'reviews',\n",
       "  'past',\n",
       "  'several_months',\n",
       "  'dear',\n",
       "  'friend',\n",
       "  'stayed_Hyatt_Regency',\n",
       "  'late_October',\n",
       "  'one_night',\n",
       "  'visiting',\n",
       "  'friend',\n",
       "  'her_husband',\n",
       "  'town',\n",
       "  'hotel',\n",
       "  'perfect',\n",
       "  'IMO',\n",
       "  'Easy_check',\n",
       "  'check_out',\n",
       "  'Lovely',\n",
       "  'clean',\n",
       "  'comfortable',\n",
       "  'rooms',\n",
       "  'great',\n",
       "  'views_city',\n",
       "  'know',\n",
       "  'area',\n",
       "  'pretty',\n",
       "  'well',\n",
       "  's',\n",
       "  'convenient',\n",
       "  'many',\n",
       "  'downtown_Chicago',\n",
       "  'attractions',\n",
       "  'dinner',\n",
       "  'went',\n",
       "  'clubing',\n",
       "  'friends',\n",
       "  'around',\n",
       "  'Division',\n",
       "  'St',\n",
       "  'problems',\n",
       "  'getting_cabs',\n",
       "  'back_forth',\n",
       "  'Hyatt',\n",
       "  's',\n",
       "  'even',\n",
       "  'public_transportation',\n",
       "  'right',\n",
       "  'near',\n",
       "  'didn_t_bother',\n",
       "  'since',\n",
       "  'needed',\n",
       "  'cabs',\n",
       "  'hotel',\n",
       "  'Parking',\n",
       "  'usual',\n",
       "  'Chicago',\n",
       "  'expensive',\n",
       "  'able',\n",
       "  'get',\n",
       "  'car',\n",
       "  'quickly',\n",
       "  'however',\n",
       "  'left',\n",
       "  'Sunday_morning',\n",
       "  'exactly',\n",
       "  'high',\n",
       "  'traffic',\n",
       "  'time',\n",
       "  'although',\n",
       "  'Bears',\n",
       "  'homegame',\n",
       "  'day',\n",
       "  'bit',\n",
       "  'busier',\n",
       "  'than_usual',\n",
       "  'think',\n",
       "  'problems',\n",
       "  'best_part',\n",
       "  'got',\n",
       "  'rate',\n",
       "  'Hotwire',\n",
       "  'downright',\n",
       "  'steal',\n",
       "  'area',\n",
       "  'Chicago',\n",
       "  'quality',\n",
       "  'hotel']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unlabelled_corpus = pickle.load(open('./input/unlabelled_corpus_raw.p','rb'))\n",
    "# true_corpus = pickle.load(open('./input/true_corpus_raw.p','rb'))\n",
    "# fake_corpus = pickle.load(open('./input/fake_corpus_raw.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stream(df, stopword_set):\n",
    "    sentence_stream = []\n",
    "    for s in df:\n",
    "        s2 = [c  for c in s if c not in stopword_set] \n",
    "        sentence_stream.append(s2)\n",
    "    return sentence_stream\n",
    "\n",
    "unlabelled_corpus = filter_stream(unlabelled_corpus, all_stop_set)\n",
    "true_corpus = filter_stream(true_corpus, all_stop_set)\n",
    "fake_corpus = filter_stream(fake_corpus, all_stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stayed',\n",
       "  'one',\n",
       "  'night',\n",
       "  'getaway',\n",
       "  'family',\n",
       "  'thursday',\n",
       "  'Triple',\n",
       "  'AAA',\n",
       "  'rate',\n",
       "  'steal',\n",
       "  'th_floor',\n",
       "  'room',\n",
       "  'complete',\n",
       "  'plasma_TV',\n",
       "  'bose',\n",
       "  'stereo',\n",
       "  'voss',\n",
       "  'evian',\n",
       "  'water',\n",
       "  'gorgeous',\n",
       "  'bathroom',\n",
       "  'tub',\n",
       "  'fine',\n",
       "  'us',\n",
       "  'Concierge',\n",
       "  'helpful',\n",
       "  'You_cannot_beat',\n",
       "  'location',\n",
       "  'flaw',\n",
       "  'breakfast',\n",
       "  'pricey',\n",
       "  'service',\n",
       "  'slow',\n",
       "  'hours',\n",
       "  'four',\n",
       "  'kids',\n",
       "  'four_adults',\n",
       "  'friday',\n",
       "  'morning',\n",
       "  'even_though',\n",
       "  'two',\n",
       "  'tables',\n",
       "  'restaurant',\n",
       "  'Food',\n",
       "  'good',\n",
       "  'worth',\n",
       "  'wait',\n",
       "  'return',\n",
       "  'heartbeat',\n",
       "  'gem',\n",
       "  'chicago'],\n",
       " ['Triple',\n",
       "  'rate',\n",
       "  'upgrade',\n",
       "  'view',\n",
       "  'room',\n",
       "  'less_than',\n",
       "  'also',\n",
       "  'included',\n",
       "  'breakfast',\n",
       "  'vouchers',\n",
       "  'great',\n",
       "  'view',\n",
       "  'river',\n",
       "  'lake',\n",
       "  'Wrigley',\n",
       "  'Bldg',\n",
       "  'Tribune',\n",
       "  'Bldg',\n",
       "  'major',\n",
       "  'restaurants',\n",
       "  'Shopping',\n",
       "  'Sightseeing',\n",
       "  'attractions',\n",
       "  'within_walking_distance',\n",
       "  'Large',\n",
       "  'room',\n",
       "  'comfortable',\n",
       "  'bed'],\n",
       " ['comes',\n",
       "  'little',\n",
       "  'late',\n",
       "  'm',\n",
       "  'finally',\n",
       "  'catching',\n",
       "  'reviews',\n",
       "  'past',\n",
       "  'several_months',\n",
       "  'dear',\n",
       "  'friend',\n",
       "  'stayed',\n",
       "  'Hyatt_Regency',\n",
       "  'late_October',\n",
       "  'one',\n",
       "  'night',\n",
       "  'visiting',\n",
       "  'friend',\n",
       "  'her_husband',\n",
       "  'town',\n",
       "  'hotel',\n",
       "  'perfect',\n",
       "  'IMO',\n",
       "  'Easy_check',\n",
       "  'check_out',\n",
       "  'Lovely',\n",
       "  'clean',\n",
       "  'comfortable',\n",
       "  'rooms',\n",
       "  'great',\n",
       "  'views',\n",
       "  'city',\n",
       "  'know',\n",
       "  'area',\n",
       "  'pretty',\n",
       "  'well',\n",
       "  's',\n",
       "  'convenient',\n",
       "  'many',\n",
       "  'downtown_Chicago',\n",
       "  'attractions',\n",
       "  'dinner',\n",
       "  'went',\n",
       "  'clubing',\n",
       "  'friends',\n",
       "  'around',\n",
       "  'Division',\n",
       "  'St',\n",
       "  'problems',\n",
       "  'getting_cabs',\n",
       "  'back',\n",
       "  'forth',\n",
       "  'Hyatt',\n",
       "  's',\n",
       "  'even',\n",
       "  'public_transportation',\n",
       "  'right',\n",
       "  'near',\n",
       "  'didn_t_bother',\n",
       "  'since',\n",
       "  'needed',\n",
       "  'cabs',\n",
       "  'hotel',\n",
       "  'Parking',\n",
       "  'usual',\n",
       "  'Chicago',\n",
       "  'expensive',\n",
       "  'able',\n",
       "  'get',\n",
       "  'car',\n",
       "  'quickly',\n",
       "  'however',\n",
       "  'left',\n",
       "  'Sunday_morning',\n",
       "  'exactly',\n",
       "  'high',\n",
       "  'traffic',\n",
       "  'time',\n",
       "  'although',\n",
       "  'Bears',\n",
       "  'homegame',\n",
       "  'day',\n",
       "  'bit',\n",
       "  'busier',\n",
       "  'than_usual',\n",
       "  'think',\n",
       "  'problems',\n",
       "  'best_part',\n",
       "  'got',\n",
       "  'rate',\n",
       "  'Hotwire',\n",
       "  'downright',\n",
       "  'steal',\n",
       "  'area',\n",
       "  'Chicago',\n",
       "  'quality',\n",
       "  'hotel']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unlabelled_corpus, open('./input/unlabelled_corpus_clean.p','wb'))\n",
    "pickle.dump(fake_corpus, open('./input/fake_corpus_clean.p','wb'))\n",
    "pickle.dump(true_corpus, open('./input/true_corpus_clean.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
