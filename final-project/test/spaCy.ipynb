{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import gensim as gs \n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,make_scorer, recall_score,precision_score,fbeta_score\n",
    "\n",
    "import gensim,logging\n",
    "from gensim.parsing import PorterStemmer\n",
    "from gensim.models import Word2Vec, Doc2Vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from wikipedia import search,page\n",
    "import multiprocessing\n",
    "import collections\n",
    "import re\n",
    "import warnings\n",
    "import spacy\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "np.random.seed(0)\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/deceptive-opinion.csv')\n",
    "\n",
    "stop_words = pd.read_csv('../input/stopwords.csv',names=['stop'])\n",
    "new_stop = stop_words.stop.map(lambda x: str.capitalize(x))\n",
    "all_stop_set = set(stop_words.stop.append(new_stop,ignore_index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate combinations of all_stop_words, bigrams\n",
    "#generate bigram\n",
    "bigram_stops=[]\n",
    "for a in all_stop_set:\n",
    "    for b in all_stop_set:\n",
    "        bigram_stop = a+\"_\"+b\n",
    "        bigram_stops.append(bigram_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_set = all_stop_set.union(bigram_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(sentdf, tokens_only=False):\n",
    "    for i, line in enumerate(sentdf):\n",
    "        if tokens_only:\n",
    "            yield list(gensim.utils.tokenize(line))\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(list(gensim.utils.tokenize(line)),[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "truedf = df[df.deceptive=='truthful'].loc[:,'text']\n",
    "fakedf = df[df.deceptive=='deceptive'].loc[:,'text']\n",
    "truedfy = df[df.deceptive=='truthful'].loc[:,'deceptive']\n",
    "fakedfy = df[df.deceptive=='deceptive'].loc[:,'deceptive']\n",
    "truedfy.replace({'truthful':1},inplace=True)\n",
    "fakedfy.replace({'deceptive':0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I stayed at The Talbott for 3 nights on business and was very pleased. The staff was friendly as can be immediately confirming the lore of the midwest. I was upgraded to a suite which was bigger than my apartment and certainly more luxurious. The free wi-fi came in handy as I needed to work remotely while there. Everything from the comfort of the bed to the staff and location made this a great stay. Oh, and I got to workout at the huge Equinox right next door for free. \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truedf.iloc[189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "# suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
    "# infix_re = re.compile(r'''[-~]''')\n",
    "# simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "# nlp.tokenizer = Tokenizer(nlp.vocab, \n",
    "#                           prefix_search = prefix_re.search,\n",
    "#                           suffix_search = suffix_re.search,\n",
    "#                           infix_finditer = infix_re.finditer,\n",
    "#                           token_match = simple_url_re.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_corpus = list(read_corpus(truedf,tokens_only=True))\n",
    "fake_corpus = list(read_corpus(fakedf,tokens_only=True))\n",
    "# def to_list(doc):\n",
    "#     return [t.text for t in doc]\n",
    "# true_corpus = [to_list(nlp(s))[:-1] for ind,s in enumerate(truedf)]\n",
    "# fake_corpus = [to_list(nlp(s))[:-1] for ind,s in enumerate(fakedf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 16:59:03,203 : INFO : collecting all words and their counts\n",
      "2018-01-25 16:59:03,205 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-25 16:59:03,659 : INFO : collected 96727 word types from a corpus of 239406 words (unigram + bigrams) and 1600 sentences\n",
      "2018-01-25 16:59:03,661 : INFO : using 96727 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-01-25 16:59:03,663 : INFO : source_vocab length 96727\n",
      "2018-01-25 16:59:04,798 : INFO : Phraser built with 828 828 phrasegrams\n",
      "2018-01-25 16:59:06,011 : INFO : collecting all words and their counts\n",
      "2018-01-25 16:59:06,013 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-25 16:59:06,430 : INFO : collected 104954 word types from a corpus of 220371 words (unigram + bigrams) and 1600 sentences\n",
      "2018-01-25 16:59:06,432 : INFO : using 104954 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-01-25 16:59:06,438 : INFO : source_vocab length 104954\n",
      "2018-01-25 16:59:07,734 : INFO : Phraser built with 1920 1920 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "phrase_list = []\n",
    "\n",
    "sentence_stream = [s for s in true_corpus + fake_corpus]\n",
    "phrases = Phrases(sentence_stream)\n",
    "bigram = Phraser(phrases)\n",
    "sentence_stream = [bigram[s] for s in sentence_stream]\n",
    "true_corpus = [bigram[s] for s in true_corpus]\n",
    "fake_corpus = [bigram[s] for s in fake_corpus]\n",
    "\n",
    "phrases = Phrases(sentence_stream)\n",
    "trigram = Phraser(phrases)\n",
    "sentence_stream = [trigram[s] for s in sentence_stream]\n",
    "true_corpus = [trigram[s] for s in true_corpus]\n",
    "fake_corpus = [trigram[s] for s in fake_corpus]\n",
    "\n",
    "def filter_stream(df, stopword_set):\n",
    "    sentence_stream = []\n",
    "    for s in df:\n",
    "        s2 = [c  for c in s if c not in stopword_set] \n",
    "        sentence_stream.append(s2)\n",
    "    return sentence_stream\n",
    "\n",
    "# sentence_stream2 = []\n",
    "# for s in sentence_stream:\n",
    "#     s2 = [c  for c in s if c not in all_stop_set] \n",
    "#     sentence_stream2.append(s2)\n",
    "\n",
    "sentence_stream2 = filter_stream(sentence_stream, all_stop_set)\n",
    "true_corpus = filter_stream(true_corpus, all_stop_set)\n",
    "fake_corpus = filter_stream(fake_corpus, all_stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We_stayed',\n",
       "  'one_night',\n",
       "  'getaway',\n",
       "  'family',\n",
       "  'thursday',\n",
       "  'Triple',\n",
       "  'AAA',\n",
       "  'rate',\n",
       "  'steal',\n",
       "  'th_floor',\n",
       "  'room',\n",
       "  'complete_with',\n",
       "  'plasma',\n",
       "  'TV',\n",
       "  'bose',\n",
       "  'stereo',\n",
       "  'voss',\n",
       "  'evian',\n",
       "  'water',\n",
       "  'gorgeous',\n",
       "  'bathroom',\n",
       "  'tub',\n",
       "  'fine',\n",
       "  'us',\n",
       "  'Concierge',\n",
       "  'very_helpful',\n",
       "  'beat',\n",
       "  'location',\n",
       "  'flaw',\n",
       "  'breakfast',\n",
       "  'pricey',\n",
       "  'service',\n",
       "  'very_slow',\n",
       "  'hours',\n",
       "  'four',\n",
       "  'kids',\n",
       "  'four',\n",
       "  'adults',\n",
       "  'friday',\n",
       "  'morning',\n",
       "  'even_though',\n",
       "  'two',\n",
       "  'tables',\n",
       "  'restaurant',\n",
       "  'Food',\n",
       "  'very_good',\n",
       "  'worth',\n",
       "  'wait',\n",
       "  'return',\n",
       "  'heartbeat',\n",
       "  'gem',\n",
       "  'chicago'],\n",
       " ['Triple',\n",
       "  'rate',\n",
       "  'upgrade',\n",
       "  'view',\n",
       "  'room',\n",
       "  'less_than',\n",
       "  'also',\n",
       "  'included',\n",
       "  'breakfast',\n",
       "  'vouchers',\n",
       "  'a_great_view',\n",
       "  'river',\n",
       "  'lake',\n",
       "  'Wrigley',\n",
       "  'Bldg',\n",
       "  'Tribune',\n",
       "  'Bldg',\n",
       "  'major',\n",
       "  'restaurants',\n",
       "  'Shopping',\n",
       "  'Sightseeing',\n",
       "  'attractions',\n",
       "  'within_walking_distance',\n",
       "  'Large',\n",
       "  'room',\n",
       "  'very_comfortable',\n",
       "  'bed'],\n",
       " ['comes',\n",
       "  'a_little',\n",
       "  'late',\n",
       "  'I_m',\n",
       "  'finally',\n",
       "  'catching',\n",
       "  'reviews',\n",
       "  'past',\n",
       "  'several',\n",
       "  'months',\n",
       "  'dear',\n",
       "  'friend',\n",
       "  'I_stayed_at',\n",
       "  'Hyatt_Regency',\n",
       "  'late',\n",
       "  'October',\n",
       "  'for_one_night',\n",
       "  'while_visiting',\n",
       "  'friend',\n",
       "  'husband',\n",
       "  'town',\n",
       "  'This_hotel_is',\n",
       "  'perfect',\n",
       "  'IMO',\n",
       "  'Easy',\n",
       "  'check_in',\n",
       "  'check_out',\n",
       "  'Lovely',\n",
       "  'clean',\n",
       "  'comfortable',\n",
       "  'rooms',\n",
       "  'great_views',\n",
       "  'city',\n",
       "  'know',\n",
       "  'area',\n",
       "  'pretty',\n",
       "  'well',\n",
       "  'it_s',\n",
       "  'very_convenient',\n",
       "  'many',\n",
       "  'downtown_Chicago',\n",
       "  'attractions',\n",
       "  'dinner',\n",
       "  'went',\n",
       "  'clubing',\n",
       "  'friends',\n",
       "  'around',\n",
       "  'Division',\n",
       "  'St',\n",
       "  'no_problems',\n",
       "  'getting',\n",
       "  'cabs',\n",
       "  'back',\n",
       "  'forth',\n",
       "  'Hyatt',\n",
       "  's',\n",
       "  'even',\n",
       "  'public_transportation',\n",
       "  'right',\n",
       "  'near',\n",
       "  'didn_t_bother',\n",
       "  'since',\n",
       "  'needed',\n",
       "  'cabs',\n",
       "  'hotel',\n",
       "  'Parking',\n",
       "  'usual',\n",
       "  'Chicago',\n",
       "  'expensive',\n",
       "  'we_were_able',\n",
       "  'get',\n",
       "  'car',\n",
       "  'quickly',\n",
       "  'however',\n",
       "  'left',\n",
       "  'Sunday',\n",
       "  'morning',\n",
       "  'not_exactly',\n",
       "  'high',\n",
       "  'traffic',\n",
       "  'time',\n",
       "  'although',\n",
       "  'Bears',\n",
       "  'homegame',\n",
       "  'day',\n",
       "  'a_bit',\n",
       "  'busier',\n",
       "  'usual',\n",
       "  'think',\n",
       "  'problems',\n",
       "  'best_part',\n",
       "  'we_got',\n",
       "  'rate',\n",
       "  'Hotwire',\n",
       "  'downright',\n",
       "  'steal',\n",
       "  'area',\n",
       "  'Chicago',\n",
       "  'quality',\n",
       "  'hotel'],\n",
       " ['Omni_Chicago',\n",
       "  'really',\n",
       "  'delivers',\n",
       "  'fronts',\n",
       "  'spaciousness',\n",
       "  'rooms',\n",
       "  'helpful',\n",
       "  'staff',\n",
       "  'prized',\n",
       "  'location',\n",
       "  'Michigan_Avenue',\n",
       "  'address',\n",
       "  'in_Chicago',\n",
       "  'requires',\n",
       "  'high',\n",
       "  'level',\n",
       "  'quality',\n",
       "  'Omni',\n",
       "  'delivers',\n",
       "  'Check_in',\n",
       "  'whole',\n",
       "  'group',\n",
       "  'people',\n",
       "  'minutes',\n",
       "  'staff',\n",
       "  'plentiful',\n",
       "  'recommendations',\n",
       "  'dining',\n",
       "  'events',\n",
       "  'rooms_are',\n",
       "  'largest',\n",
       "  'll',\n",
       "  'find',\n",
       "  'price',\n",
       "  'range',\n",
       "  'in_Chicago',\n",
       "  'Even',\n",
       "  'standard',\n",
       "  'room',\n",
       "  'separate',\n",
       "  'living',\n",
       "  'area',\n",
       "  'work',\n",
       "  'desk',\n",
       "  'fitness_center',\n",
       "  'free',\n",
       "  'weights',\n",
       "  'weight',\n",
       "  'machines',\n",
       "  'two',\n",
       "  'rows',\n",
       "  'cardio',\n",
       "  'equipment',\n",
       "  'shared',\n",
       "  'room',\n",
       "  'others',\n",
       "  'feel',\n",
       "  'cramped',\n",
       "  'way',\n",
       "  'All_in_all',\n",
       "  'great',\n",
       "  'property']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_corpus[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(true_corpus, open('../input/true_corpus_clean.p','wb'))\n",
    "pickle.dump(fake_corpus, open('../input/fake_corpus_clean.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound = []\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "# raw_corpus = true_corpus + fake_corpus\n",
    "for s in sentence_stream2:\n",
    "    sent = sia.polarity_scores(' '.join(s))\n",
    "    compound.append(sent['compound'])\n",
    "    neg.append(sent['neg'])\n",
    "    neu.append(sent['neu'])\n",
    "    pos.append(sent['pos'])\n",
    "\n",
    "vader_sent = pd.DataFrame({'compound':compound, 'neg':neg, 'neu':neu, 'pos':pos})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "lexicon_results = pd.DataFrame(columns=lexicon.cats)\n",
    "for ind, s in enumerate(sentence_stream2):\n",
    "    lexicon_results = lexicon_results.append(pd.Series([np.nan]), ignore_index=True)\n",
    "    results = (lexicon.analyze(s))\n",
    "    if (ind % 100 == 0):\n",
    "        print(ind)\n",
    "    for k in results.keys():\n",
    "        lexicon_results[k].iloc[ind] = results[k]\n",
    "\n",
    "lexicon_results.drop(columns=[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>help</th>\n",
       "      <th>office</th>\n",
       "      <th>dance</th>\n",
       "      <th>money</th>\n",
       "      <th>wedding</th>\n",
       "      <th>domestic_work</th>\n",
       "      <th>sleep</th>\n",
       "      <th>medical_emergency</th>\n",
       "      <th>cold</th>\n",
       "      <th>hate</th>\n",
       "      <th>...</th>\n",
       "      <th>weapon</th>\n",
       "      <th>children</th>\n",
       "      <th>monster</th>\n",
       "      <th>ocean</th>\n",
       "      <th>giving</th>\n",
       "      <th>contentment</th>\n",
       "      <th>writing</th>\n",
       "      <th>rural</th>\n",
       "      <th>positive_emotion</th>\n",
       "      <th>musical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   help  office  dance  money  wedding  domestic_work  sleep  \\\n",
       "0   0.0     1.0    0.0    2.0      1.0            3.0    1.0   \n",
       "1   0.0     2.0    0.0    0.0      0.0            0.0    1.0   \n",
       "2   2.0     0.0    0.0    1.0      1.0            2.0    3.0   \n",
       "3   1.0     5.0    0.0    1.0      0.0            0.0    0.0   \n",
       "4   0.0     1.0    0.0    0.0      0.0            0.0    0.0   \n",
       "5   1.0     4.0    0.0    0.0      0.0            2.0    3.0   \n",
       "6   2.0     4.0    1.0    0.0      0.0            1.0    0.0   \n",
       "\n",
       "   medical_emergency  cold  hate   ...     weapon  children  monster  ocean  \\\n",
       "0                0.0   0.0   0.0   ...        0.0       1.0      0.0    1.0   \n",
       "1                0.0   0.0   0.0   ...        0.0       0.0      0.0    0.0   \n",
       "2                0.0   0.0   0.0   ...        0.0       1.0      0.0    0.0   \n",
       "3                0.0   1.0   0.0   ...        0.0       0.0      0.0    0.0   \n",
       "4                0.0   0.0   0.0   ...        0.0       0.0      0.0    0.0   \n",
       "5                0.0   2.0   0.0   ...        0.0       0.0      0.0    0.0   \n",
       "6                0.0   0.0   0.0   ...        0.0       0.0      0.0    0.0   \n",
       "\n",
       "   giving  contentment  writing  rural  positive_emotion  musical  \n",
       "0     2.0          0.0      0.0    0.0               1.0      0.0  \n",
       "1     0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "2     0.0          0.0      0.0    0.0               2.0      0.0  \n",
       "3     0.0          1.0      0.0    1.0               1.0      0.0  \n",
       "4     3.0          0.0      1.0    0.0               0.0      0.0  \n",
       "5     1.0          0.0      0.0    0.0               2.0      0.0  \n",
       "6     2.0          0.0      0.0    0.0               0.0      1.0  \n",
       "\n",
       "[7 rows x 194 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_results.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from gensim import corpora, models, similarities\n",
    "\n",
    "# unique_dict = corpora.Dictionary(sentence_stream2)\n",
    "# dict_corpus = [dictionary.doc2bow(s) for s in sentence_stream2]\n",
    "# tfidf = models.TfidfModel(dict_corpus)\n",
    "# dict_corpus_tfidf = tfidf[dict_corpus]\n",
    "# index = similarities.MatrixSimilarity(dict_corpus_tfidf)\n",
    "# sims = index[dict_corpus_tfidf]\n",
    "# for ind,a in enumerate(dict_corpus_tfidf):\n",
    "#     print('1: ' + str(len(dict_corpus_tfidf[ind])))\n",
    "#     print('2: ' + str(len(sentence_stream[ind])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_fake_corpus = []\n",
    "for ind,s in enumerate(sentence_stream2):\n",
    "    true_fake_corpus.append(gensim.models.doc2vec.TaggedDocument(s,[ind]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 17:08:28,239 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-01-25 17:08:28,400 : INFO : built Dictionary(11790 unique tokens: ['We_stayed', 'one_night', 'getaway', 'family', 'thursday']...) from 1600 documents (total 109981 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(sentence_stream2)\n",
    "corpus_bow = [dictionary.doc2bow(s) for s in sentence_stream2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 16:24:20,529 : INFO : using autotuned alpha, starting with [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "2018-01-25 16:24:20,534 : INFO : using serial LDA version on this node\n",
      "2018-01-25 16:24:24,971 : INFO : running online (multi-pass) LDA training, 100 topics, 5 passes over the supplied corpus of 100 documents, updating model once every 100 documents, evaluating perplexity every 100 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-01-25 16:24:24,973 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2018-01-25 16:24:30,135 : INFO : -597.266 per-word bound, 623765544622707642014164182422772629796457818748685720487405438420177196883473316715526970096100318765830385490424056426726774681659919515834871900292577988231149863414296626593792.0 perplexity estimate based on a held-out corpus of 100 documents with 6199 words\n",
      "2018-01-25 16:24:30,137 : INFO : PROGRESS: pass 0, at document #100/100\n",
      "2018-01-25 16:24:30,496 : INFO : optimized alpha [0.009641199502725373, 0.009955338119765489, 0.00974685859976447, 0.010242460597468885, 0.010055122191635523, 0.009747587323695123, 0.009646786131059402, 0.010056570241661886, 0.009750473841156421, 0.01005909116211262, 0.0095429086397463, 0.009645899694031635, 0.009956306281321452, 0.010363422920944413, 0.009849866507135369, 0.010060878659493538, 0.009955662171458234, 0.00974975859749759, 0.010469141756346126, 0.010060182940156773, 0.009646352905158447, 0.010162602469895272, 0.00995145185220032, 0.009542908688171018, 0.0095429086397463, 0.010783668841133385, 0.009953373861798348, 0.010012326937471644, 0.009921368850983978, 0.00964566403060536, 0.00974825272715605, 0.009750001026762538, 0.0095429086397463, 0.010267262602587453, 0.009646032962667991, 0.00984544752987578, 0.009954745697252524, 0.009849243725829158, 0.00964688596546675, 0.009854114739771892, 0.009644750569622527, 0.0095429086397463, 0.009854135748443052, 0.009749095000724925, 0.009958380370061665, 0.009647681071710493, 0.00985062864260101, 0.010056446353795217, 0.009646157806762578, 0.009646106170383176, 0.010060121544950963, 0.0099567481758245, 0.009956697090650616, 0.009841197611115756, 0.009665128404567985, 0.00984957371522362, 0.009953527723100644, 0.009851432473575393, 0.010058334804838937, 0.0095429086397463, 0.009837610382504976, 0.009953985070785177, 0.009851668399066316, 0.009837339400117068, 0.00995629338637868, 0.0095429086397463, 0.010152341187043935, 0.009952747225663462, 0.009956421786996603, 0.00984445371436211, 0.009852592249275798, 0.010157910783970622, 0.009847949993447567, 0.0095429086397463, 0.0097477048434762, 0.009645928523954633, 0.009955859664899086, 0.009959481183060073, 0.010059635334468325, 0.009743457365389507, 0.00964723306835445, 0.009740956896319515, 0.010057437183389988, 0.00995623399609314, 0.009644680408248086, 0.009852213805307552, 0.009748717948750764, 0.0095429086397463, 0.009743351970252997, 0.009748274057788536, 0.00964720710985137, 0.010162057906858646, 0.009749232399976616, 0.009747789294732898, 0.010056340104190643, 0.009746280772425012, 0.009854022212341978, 0.009748638707720216, 0.01046632434652451, 0.010270113504604937]\n",
      "2018-01-25 16:24:30,674 : INFO : topic #32 (0.010): 0.000*\"Conveniently\" + 0.000*\"renowned\" + 0.000*\"smokers\" + 0.000*\"Wife\" + 0.000*\"Base\" + 0.000*\"ambulance\" + 0.000*\"THREE\" + 0.000*\"soundproofing\" + 0.000*\"hesitate\" + 0.000*\"rely\"\n",
      "2018-01-25 16:24:30,676 : INFO : topic #41 (0.010): 0.000*\"Conveniently\" + 0.000*\"renowned\" + 0.000*\"smokers\" + 0.000*\"Wife\" + 0.000*\"Base\" + 0.000*\"ambulance\" + 0.000*\"THREE\" + 0.000*\"soundproofing\" + 0.000*\"hesitate\" + 0.000*\"rely\"\n",
      "2018-01-25 16:24:30,678 : INFO : topic #98 (0.010): 0.016*\"room\" + 0.013*\"hotel\" + 0.005*\"quite\" + 0.005*\"us\" + 0.005*\"great\" + 0.004*\"view\" + 0.004*\"separate\" + 0.004*\"hotels\" + 0.004*\"rate\" + 0.004*\"s\"\n",
      "2018-01-25 16:24:30,681 : INFO : topic #18 (0.010): 0.017*\"room\" + 0.016*\"hotel\" + 0.012*\"great\" + 0.009*\"Chicago\" + 0.008*\"Omni\" + 0.007*\"wonderful\" + 0.007*\"bathroom\" + 0.006*\"restaurant\" + 0.006*\"two\" + 0.006*\"reviews\"\n",
      "2018-01-25 16:24:30,684 : INFO : topic #25 (0.011): 0.022*\"great\" + 0.015*\"room\" + 0.009*\"hotel\" + 0.008*\"staff\" + 0.007*\"huge\" + 0.007*\"wonderful\" + 0.007*\"everything\" + 0.006*\"Chicago\" + 0.006*\"really\" + 0.005*\"restaurant\"\n",
      "2018-01-25 16:24:30,697 : INFO : topic diff=94.248405, rho=1.000000\n",
      "2018-01-25 16:24:31,217 : INFO : -12.847 per-word bound, 7366.2 perplexity estimate based on a held-out corpus of 100 documents with 6199 words\n",
      "2018-01-25 16:24:31,221 : INFO : PROGRESS: pass 1, at document #100/100\n",
      "2018-01-25 16:24:31,355 : INFO : optimized alpha [0.00939289048006604, 0.00969058682873275, 0.009493078093228685, 0.01002375448140887, 0.009906355537892427, 0.009552772827987381, 0.009456153326786109, 0.009846837909032093, 0.009613664778172899, 0.009910049490456449, 0.009299636031008062, 0.009397348541859897, 0.009811335044259224, 0.010325988435096063, 0.00964978230656598, 0.009974729526875703, 0.009750843134774536, 0.009554676995268315, 0.010492200744575077, 0.009912210554012552, 0.00945577903165089, 0.01013287474630006, 0.009686907189925351, 0.009299636076964208, 0.009299636031008062, 0.010993163296586467, 0.009748577308921407, 0.009925951636250046, 0.009717638374942548, 0.009397125021401269, 0.00955302434090033, 0.009554331792718114, 0.009299636031008062, 0.010231815669930087, 0.009397474943466882, 0.009645960602530809, 0.009810364385010047, 0.0095901044178744, 0.009398283990728073, 0.009713183246652882, 0.009396258625281302, 0.009299636031008062, 0.009713727581418951, 0.009495198039598371, 0.009814351420517455, 0.009457628756120336, 0.00965099724544297, 0.009967057396286942, 0.009397593354543543, 0.009397544378914855, 0.00990927840950515, 0.009812282852115469, 0.009751926086784034, 0.009641248631387883, 0.009415585460593406, 0.009650159482491653, 0.00974898789640993, 0.009592177996376818, 0.009848927729864508, 0.009299636031008062, 0.009579082777715579, 0.009689305729092074, 0.009592401506228483, 0.00957882603548281, 0.009751182424525775, 0.009299636031008062, 0.009938001036039564, 0.009748103040968301, 0.009751765220370896, 0.009585566361755038, 0.009593276735724585, 0.01000521323221859, 0.009647983430750719, 0.009299636031008062, 0.009552587034084034, 0.009397375886250649, 0.009750946466494655, 0.009875440507933469, 0.009969446151435723, 0.009489853917841356, 0.009457025345851583, 0.00948748357698226, 0.009907699028768247, 0.009751567318123008, 0.0093961920786942, 0.009592918209189403, 0.009553081497150935, 0.009299636031008062, 0.009548827210050627, 0.009552989112446363, 0.009456728137046854, 0.01000878780304855, 0.009553713107026272, 0.00949396032799926, 0.009968070034194936, 0.009492530350359811, 0.00971351124751829, 0.00949476550899804, 0.010173702231062915, 0.010236298458896391]\n",
      "2018-01-25 16:24:31,528 : INFO : topic #32 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:31,531 : INFO : topic #59 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:31,533 : INFO : topic #13 (0.010): 0.017*\"hotel\" + 0.014*\"great\" + 0.013*\"room\" + 0.007*\"Omni\" + 0.007*\"excellent\" + 0.007*\"Conrad\" + 0.007*\"stay\" + 0.007*\"loved\" + 0.006*\"wonderful\" + 0.006*\"Just\"\n",
      "2018-01-25 16:24:31,536 : INFO : topic #18 (0.010): 0.020*\"room\" + 0.015*\"hotel\" + 0.013*\"great\" + 0.013*\"Chicago\" + 0.009*\"wonderful\" + 0.009*\"Omni\" + 0.008*\"two\" + 0.008*\"restaurant\" + 0.008*\"reviews\" + 0.007*\"bathroom\"\n",
      "2018-01-25 16:24:31,538 : INFO : topic #25 (0.011): 0.022*\"great\" + 0.016*\"room\" + 0.009*\"staff\" + 0.009*\"hotel\" + 0.008*\"huge\" + 0.008*\"everything\" + 0.008*\"wonderful\" + 0.007*\"best\" + 0.007*\"really\" + 0.006*\"Omni\"\n",
      "2018-01-25 16:24:31,553 : INFO : topic diff=1.454415, rho=0.577350\n",
      "2018-01-25 16:24:32,042 : INFO : -11.184 per-word bound, 2326.1 perplexity estimate based on a held-out corpus of 100 documents with 6199 words\n",
      "2018-01-25 16:24:32,044 : INFO : PROGRESS: pass 2, at document #100/100\n",
      "2018-01-25 16:24:32,152 : INFO : optimized alpha [0.009187370971149932, 0.009471835491545788, 0.009283151566914397, 0.009841814690793546, 0.009781200406481085, 0.009390229974056495, 0.009297025039473895, 0.009672177253606749, 0.009498357266528461, 0.009784670155557373, 0.009098176573841545, 0.009191633926721654, 0.00968930523736062, 0.010292436415806626, 0.009482963210071698, 0.009901107403738965, 0.0095801702382377, 0.00939192027399581, 0.010509716056893856, 0.009787691715317575, 0.009296697923262334, 0.010106011581437957, 0.00946832192141821, 0.009098176617806859, 0.009098176573841545, 0.011171578464264276, 0.00957792454302677, 0.009852142975394999, 0.009547861517173111, 0.009191420190796449, 0.009390152755187333, 0.009391109098103175, 0.009098176573841545, 0.010200322164739173, 0.009191754795037548, 0.009479618821739946, 0.009688811235726435, 0.009375865390143434, 0.009192528424104344, 0.009594479738805903, 0.009190591718679128, 0.009098176573841545, 0.009595446429768482, 0.009285177756966825, 0.009693079976909907, 0.009298960282029981, 0.009484640691406842, 0.009890755228332443, 0.009191868022544944, 0.009191821190883765, 0.009730960543094028, 0.00969066062064628, 0.0095812901103625, 0.009474517593952516, 0.009209071746746797, 0.009483871127544227, 0.009578537991958513, 0.009377846326562642, 0.009674522137839137, 0.009098176573841545, 0.009365335832179264, 0.009470612217375359, 0.009378059849381557, 0.00936509054595403, 0.009580278047679375, 0.009098176573841545, 0.009759601455191903, 0.009577571408634732, 0.009581220024355108, 0.009371530012703183, 0.009378895968986552, 0.009876822518027354, 0.009481432777371549, 0.009098176573841545, 0.00938980363246476, 0.009191660074104865, 0.009580198908472755, 0.009803611006070865, 0.009892604358611118, 0.009280069943144127, 0.009298233959894884, 0.009277804372143069, 0.009781529694196834, 0.009581014318573906, 0.009190528084833291, 0.009378553463747753, 0.009389886047473487, 0.009098176573841545, 0.0093865182033469, 0.00939007267751312, 0.009297721513011701, 0.00987987457965265, 0.009390609996865272, 0.009283994786691136, 0.009892686054591926, 0.009282628045008963, 0.009595147196784101, 0.009284764357075854, 0.009932599628341083, 0.010206173679925256]\n",
      "2018-01-25 16:24:32,338 : INFO : topic #41 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:32,340 : INFO : topic #87 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:32,342 : INFO : topic #13 (0.010): 0.018*\"hotel\" + 0.013*\"great\" + 0.013*\"room\" + 0.008*\"loved\" + 0.008*\"excellent\" + 0.007*\"Conrad\" + 0.007*\"Just\" + 0.007*\"Loved\" + 0.007*\"just_returned_from\" + 0.007*\"attached\"\n",
      "2018-01-25 16:24:32,345 : INFO : topic #18 (0.011): 0.022*\"room\" + 0.016*\"hotel\" + 0.014*\"Chicago\" + 0.014*\"great\" + 0.009*\"wonderful\" + 0.009*\"Omni\" + 0.009*\"two\" + 0.008*\"restaurant\" + 0.008*\"reviews\" + 0.007*\"morning\"\n",
      "2018-01-25 16:24:32,347 : INFO : topic #25 (0.011): 0.022*\"great\" + 0.017*\"room\" + 0.010*\"staff\" + 0.009*\"hotel\" + 0.008*\"wonderful\" + 0.008*\"huge\" + 0.008*\"everything\" + 0.008*\"Omni\" + 0.007*\"best\" + 0.007*\"really\"\n",
      "2018-01-25 16:24:32,362 : INFO : topic diff=0.779335, rho=0.500000\n",
      "2018-01-25 16:24:32,881 : INFO : -10.277 per-word bound, 1240.6 perplexity estimate based on a held-out corpus of 100 documents with 6199 words\n",
      "2018-01-25 16:24:32,883 : INFO : PROGRESS: pass 3, at document #100/100\n",
      "2018-01-25 16:24:32,984 : INFO : optimized alpha [0.009010466987504488, 0.009283808721771246, 0.009102540336546996, 0.009684544816576256, 0.009671998562668638, 0.009249383305882292, 0.009159083556778637, 0.009521066173572047, 0.009397592667337095, 0.009675277904465507, 0.008924690359045811, 0.009014565759862767, 0.009582780905961076, 0.010261734605651779, 0.009338497664236238, 0.009836002375291992, 0.009432434683452806, 0.009250894741042374, 0.010523638622468615, 0.0096790218454081, 0.009158795891669756, 0.010081286020726929, 0.009280434602398711, 0.00892469040133467, 0.008924690359045811, 0.01132845136136659, 0.009430207408209454, 0.009786895079340397, 0.009400893611771786, 0.009014360257562708, 0.009249033142546017, 0.009249697231593655, 0.008924690359045811, 0.010171393568349589, 0.009014681971953887, 0.009335551818029045, 0.009582688493947295, 0.009191629013071297, 0.009015425796965107, 0.009490804863003218, 0.009013563698388115, 0.008924690359045811, 0.009492127003659135, 0.009104487686616002, 0.009587192301197074, 0.009161399620564236, 0.009340581660840149, 0.009823363455911366, 0.00901479083752879, 0.009014745809998838, 0.009532506781782552, 0.009584478768129599, 0.00943358429280055, 0.009330124482411278, 0.009031331188189011, 0.009339845248548058, 0.009430988533654063, 0.009193532101236157, 0.00952362043088461, 0.008924690359045811, 0.00918151297731018, 0.009282634004230236, 0.009193737231901483, 0.00918127731855627, 0.009432350553489106, 0.008924690359045811, 0.009605326032872938, 0.009429954950924132, 0.009433589508329043, 0.009187463953039912, 0.009194540487400222, 0.009764849600384937, 0.009337223328000327, 0.008924690359045811, 0.009248757508606873, 0.009014590899972356, 0.00943240144847794, 0.009740100017891964, 0.009824787182779975, 0.009099578587203606, 0.009160571644553828, 0.00909740112099993, 0.009671269951787548, 0.009433377492846447, 0.009013502515614235, 0.009194211444858085, 0.009248497710284518, 0.008924690359045811, 0.009245868497359614, 0.009248915881202798, 0.00915988065102925, 0.009767408346292231, 0.009249297793619632, 0.009103350748344967, 0.009826077984321708, 0.00910203718232317, 0.009491758053531696, 0.009104090373700597, 0.009725837497567159, 0.01017863042711278]\n",
      "2018-01-25 16:24:33,183 : INFO : topic #10 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:33,184 : INFO : topic #65 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:33,187 : INFO : topic #13 (0.010): 0.019*\"hotel\" + 0.013*\"great\" + 0.013*\"room\" + 0.008*\"loved\" + 0.008*\"excellent\" + 0.008*\"Conrad\" + 0.008*\"Just\" + 0.008*\"Loved\" + 0.008*\"just_returned_from\" + 0.007*\"attached\"\n",
      "2018-01-25 16:24:33,189 : INFO : topic #18 (0.011): 0.022*\"room\" + 0.017*\"hotel\" + 0.015*\"Chicago\" + 0.014*\"great\" + 0.009*\"wonderful\" + 0.009*\"Omni\" + 0.009*\"two\" + 0.008*\"restaurant\" + 0.008*\"reviews\" + 0.008*\"morning\"\n",
      "2018-01-25 16:24:33,191 : INFO : topic #25 (0.011): 0.022*\"great\" + 0.017*\"room\" + 0.010*\"staff\" + 0.009*\"Omni\" + 0.009*\"wonderful\" + 0.008*\"hotel\" + 0.008*\"huge\" + 0.008*\"everything\" + 0.008*\"best\" + 0.007*\"really\"\n",
      "2018-01-25 16:24:33,205 : INFO : topic diff=0.602715, rho=0.447214\n",
      "2018-01-25 16:24:33,728 : INFO : -9.743 per-word bound, 857.2 perplexity estimate based on a held-out corpus of 100 documents with 6199 words\n",
      "2018-01-25 16:24:33,730 : INFO : PROGRESS: pass 4, at document #100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 16:24:33,832 : INFO : optimized alpha [0.008854324802392559, 0.009118050713554862, 0.00894319120190614, 0.00954522474640571, 0.009574472879825168, 0.00912434890063259, 0.00903658704806465, 0.009387100105339236, 0.00930748151727069, 0.009577586200050671, 0.0087715064862365, 0.008858281505989517, 0.00948760806283971, 0.010233253000079885, 0.009210316936522505, 0.009777175676536807, 0.00930140413149752, 0.009125706491368419, 0.01053498883866705, 0.009581953280806155, 0.009036333309267452, 0.010058206283983314, 0.009114797042171852, 0.008771506527073846, 0.0087715064862365, 0.011469263924666465, 0.009299193950250749, 0.009727956566821683, 0.009270537786852292, 0.008858083128203183, 0.00912376554342443, 0.009124179357920807, 0.0087715064862365, 0.010102282370207613, 0.008858393689077534, 0.00920771338163465, 0.009487863211762091, 0.009029145867357953, 0.008859111724894756, 0.009398135734070908, 0.00885731418322809, 0.0087715064862365, 0.009399765177641212, 0.008945070373147988, 0.009492568636892802, 0.00903922768922451, 0.009212763487216, 0.009762535808945417, 0.008858498780301526, 0.008858455313873881, 0.009357750368446249, 0.009489601600859435, 0.009302578421086907, 0.009202003772553693, 0.008874465150552218, 0.009212039375504693, 0.009300117744670124, 0.009030981674821022, 0.009389831362740526, 0.0087715064862365, 0.009019387234524341, 0.009116917935585191, 0.009031179552581772, 0.00901915989698464, 0.009301156202393936, 0.0087715064862365, 0.009468608464896649, 0.009299027596640879, 0.009302647920109989, 0.009025128007927147, 0.009031954405428953, 0.009664889494727352, 0.009209262173850275, 0.0087715064862365, 0.009123552910883563, 0.008858305774511074, 0.009301318069562924, 0.009682710725428537, 0.009763592308181156, 0.008940333118659265, 0.009038313187348, 0.008938231845845544, 0.0095727267043699, 0.009302430647998364, 0.008857255121367553, 0.009031636997941378, 0.00912300066259913, 0.0087715064862365, 0.009121004599382495, 0.009123616556901757, 0.009037469789263725, 0.00966699284277664, 0.009123865366564182, 0.008943973242083443, 0.009765936316525756, 0.00894270566128492, 0.009399335993896505, 0.008944686971770076, 0.0095439217520362, 0.010153034253948897]\n",
      "2018-01-25 16:24:34,050 : INFO : topic #41 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:34,053 : INFO : topic #73 (0.009): 0.000*\"hotel\" + 0.000*\"room\" + 0.000*\"great\" + 0.000*\"Chicago\" + 0.000*\"staff\" + 0.000*\"stay\" + 0.000*\"bed\" + 0.000*\"this_hotel\" + 0.000*\"nice\" + 0.000*\"restaurant\"\n",
      "2018-01-25 16:24:34,055 : INFO : topic #13 (0.010): 0.020*\"hotel\" + 0.013*\"great\" + 0.013*\"room\" + 0.008*\"excellent\" + 0.008*\"Conrad\" + 0.008*\"Just\" + 0.008*\"Loved\" + 0.008*\"just_returned_from\" + 0.008*\"loved\" + 0.008*\"attached\"\n",
      "2018-01-25 16:24:34,057 : INFO : topic #18 (0.011): 0.023*\"room\" + 0.017*\"hotel\" + 0.015*\"Chicago\" + 0.014*\"great\" + 0.009*\"wonderful\" + 0.009*\"Omni\" + 0.009*\"two\" + 0.009*\"restaurant\" + 0.008*\"reviews\" + 0.008*\"morning\"\n",
      "2018-01-25 16:24:34,060 : INFO : topic #25 (0.011): 0.022*\"great\" + 0.017*\"room\" + 0.010*\"staff\" + 0.010*\"Omni\" + 0.009*\"wonderful\" + 0.008*\"hotel\" + 0.008*\"everything\" + 0.008*\"huge\" + 0.008*\"best\" + 0.007*\"really\"\n",
      "2018-01-25 16:24:34,075 : INFO : topic diff=0.477000, rho=0.408248\n"
     ]
    }
   ],
   "source": [
    "num_topics = 100\n",
    "chunksize = 400\n",
    "passes = 5\n",
    "\n",
    "model = models.LdaModel(corpus_bow[:100], id2word=dictionary, num_topics=num_topics,alpha = 'auto',eta='auto',random_state=0, chunksize=chunksize, passes=passes)\n",
    "\n",
    "# model.update(corpus_bow[500:len(corpus_bow)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dists = np.zeros([len(sentence_stream2),num_topics])\n",
    "    \n",
    "for i,item in enumerate(corpus_bow):       \n",
    "    dists = model.get_document_topics(item)        \n",
    "    indices = list(dict(dists).keys())        \n",
    "    vals = list(dict(dists).values())        \n",
    "    topic_dists[i,indices] = vals\n",
    "\n",
    "topic_dists = pd.DataFrame(topic_dists, columns = ['topic'+str(a) for a in range(num_topics)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.967205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic0  topic1  topic2    topic3  topic4  topic5  topic6  topic7  topic8  \\\n",
       "0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0  0.000000     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0  0.000000     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0  0.000000     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0  0.971327     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   topic9   ...     topic90  topic91   topic92  topic93  topic94  topic95  \\\n",
       "0     0.0   ...         0.0      0.0  0.000000      0.0      0.0      0.0   \n",
       "1     0.0   ...         0.0      0.0  0.967205      0.0      0.0      0.0   \n",
       "2     0.0   ...         0.0      0.0  0.000000      0.0      0.0      0.0   \n",
       "3     0.0   ...         0.0      0.0  0.000000      0.0      0.0      0.0   \n",
       "4     0.0   ...         0.0      0.0  0.000000      0.0      0.0      0.0   \n",
       "\n",
       "   topic96  topic97  topic98  topic99  \n",
       "0      0.0      0.0      0.0      0.0  \n",
       "1      0.0      0.0      0.0      0.0  \n",
       "2      0.0      0.0      0.0      0.0  \n",
       "3      0.0      0.0      0.0      0.0  \n",
       "4      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 17:08:28,613 : INFO : collecting all words and their counts\n",
      "2018-01-25 17:08:28,616 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-01-25 17:08:28,643 : INFO : collected 11790 word types and 1600 unique tags from a corpus of 1600 examples and 109981 words\n",
      "2018-01-25 17:08:28,645 : INFO : Loading a fresh vocabulary\n",
      "2018-01-25 17:08:28,670 : INFO : min_count=1 retains 11790 unique words (100% of original 11790, drops 0)\n",
      "2018-01-25 17:08:28,672 : INFO : min_count=1 leaves 109981 word corpus (100% of original 109981, drops 0)\n",
      "2018-01-25 17:08:28,754 : INFO : deleting the raw counts dictionary of 11790 items\n",
      "2018-01-25 17:08:28,757 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2018-01-25 17:08:28,759 : INFO : downsampling leaves estimated 104543 word corpus (95.1% of prior 109981)\n",
      "2018-01-25 17:08:28,761 : INFO : estimated required memory for 11790 words and 100 dimensions: 15967000 bytes\n",
      "2018-01-25 17:08:28,802 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "turk_model = gensim.models.doc2vec.Doc2Vec(dm=0, size=100,min_count=1, window=5,workers=cores, seed=8, negative=5)\n",
    "turk_model.build_vocab(true_fake_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 17:08:28,962 : INFO : training model with 8 workers on 11790 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-25 17:08:29,608 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-01-25 17:08:29,628 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-01-25 17:08:29,643 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-01-25 17:08:29,665 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-01-25 17:08:29,672 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-25 17:08:29,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-25 17:08:29,680 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-25 17:08:29,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-25 17:08:29,685 : INFO : training on 549905 raw words (530533 effective words) took 0.7s, 743384 effective words/s\n",
      "2018-01-25 17:08:29,687 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "530533"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turk_model.train(true_fake_corpus, total_examples=turk_model.corpus_count, epochs=turk_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "true_fake_vec = pd.DataFrame([turk_model.infer_vector(s.words) for s in true_fake_corpus])\n",
    "# vader_sent = vader_sent.apply(np.square)\n",
    "true_fake_vec2 = pd.concat([true_fake_vec, vader_sent, lexicon_results], axis=1)\n",
    "\n",
    "# ss = StandardScaler()\n",
    "# true_fake_vec_all = ss.fit_transform(true_fake_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y = pd.concat([truedfy, fakedfy], axis= 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(true_fake_vec, all_y, train_size=0.75, random_state=8)\n",
    "Xv_train, Xv_test, yv_train, yv_test = train_test_split(vader_sent, all_y,train_size=0.75, random_state=8)\n",
    "Xe_train, Xe_test, ye_train, ye_test = train_test_split(lexicon_results, all_y, train_size=0.75, random_state=8)\n",
    "# Xlda_train, Xlda_test, ylda_train, ylda_test = train_test_split(topic_dists, all_y, train_size=0.75, random_state=8)\n",
    "Xa_train, Xa_test, ya_train, ya_test = train_test_split(true_fake_vec2, all_y,train_size=0.75, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def run_test(X, y, cv_val, scoring):\n",
    "    gnb = GaussianNB()\n",
    "    dtree = DecisionTreeClassifier()\n",
    "    svm2 = svm.SVC(random_state=8)\n",
    "    xg = XGBClassifier()\n",
    "    # logreg_cv = linear_model.LogisticRegressionCV(Cs=100, cv=5, penalty='l1',scoring='accuracy',solver='liblinear',n_jobs=-1)\n",
    "    print('Gaussian NB:')\n",
    "    scorelist = cross_val_score(gnb, X, y, cv=cv_val, scoring=scoring,n_jobs=-1)\n",
    "    print(scorelist, np.mean(scorelist))\n",
    "    print('DecisionTree')\n",
    "    scorelist = cross_val_score(dtree, X, y, cv=cv_val, scoring=scoring,n_jobs=-1)\n",
    "    print(scorelist, np.mean(scorelist))\n",
    "    print('SVM:')\n",
    "    scorelist = cross_val_score(svm2, X, y, cv=cv_val, scoring=scoring,n_jobs=-1)\n",
    "    print(scorelist, np.mean(scorelist))\n",
    "    print('XGB Default:')\n",
    "    scorelist = cross_val_score(xg, X, y, cv=cv_val, scoring=scoring,n_jobs=-1)\n",
    "    print(scorelist, np.mean(scorelist))\n",
    "    # print('Logistics Regression:')\n",
    "    # scorelist = cross_val_score(logreg_cv, X_train, y_train, cv=5, scoring='f1', n_jobs=-1)\n",
    "    # print(scorelist, np.mean(scorelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB:\n",
      "[0.54634146 0.53968254 0.69343066 0.43850267 0.5049505 ] 0.5445815657755554\n",
      "DecisionTree\n",
      "[0.63709677 0.63453815 0.55645161 0.57758621 0.58634538] 0.5984036256259744\n",
      "SVM:\n",
      "[0.74789916 0.65546218 0.73684211 0.70995671 0.65822785] 0.7016776015717897\n",
      "XGB Default:\n",
      "[0.76422764 0.66666667 0.68273092 0.65437788 0.66949153] 0.6874989276491859\n"
     ]
    }
   ],
   "source": [
    "run_test(Xe_train, ye_train, 5, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB:\n",
      "[0.59385666 0.57342657 0.63272727 0.58992806 0.60839161] 0.5996660334779028\n",
      "DecisionTree\n",
      "[0.52       0.42060086 0.47161572 0.50622407 0.56153846] 0.49599582136432385\n",
      "SVM:\n",
      "[0.6402439  0.62420382 0.64126984 0.64353312 0.64797508] 0.6394451532549856\n",
      "XGB Default:\n",
      "[0.55833333 0.5021645  0.5511811  0.52542373 0.56903766] 0.541228064715473\n"
     ]
    }
   ],
   "source": [
    "run_test(Xv_train, yv_train, 5, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB:\n",
      "[0.609319   0.55970149 0.67375887 0.57553957 0.63481229] 0.6106262418472109\n",
      "DecisionTree\n",
      "[0.59414226 0.47058824 0.53877551 0.47533632 0.47863248] 0.5114949612829718\n",
      "SVM:\n",
      "[0.66850829 0.67036011 0.67036011 0.67036011 0.66852368] 0.6696224593166027\n",
      "XGB Default:\n",
      "[0.49392713 0.55737705 0.47457627 0.53941909 0.48535565] 0.5101310363090672\n"
     ]
    }
   ],
   "source": [
    "run_test(X_train, y_train, 5, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB:\n",
      "[0.58015267 0.56573705 0.61643836 0.40236686 0.59778598] 0.5524961842956082\n",
      "DecisionTree\n",
      "[0.50209205 0.5982906  0.5483871  0.52589641 0.62761506] 0.5604562444756265\n",
      "SVM:\n",
      "[0.66850829 0.67036011 0.67036011 0.67036011 0.66852368] 0.6696224593166027\n",
      "XGB Default:\n",
      "[0.64615385 0.58436214 0.60162602 0.61904762 0.64957265] 0.6201524541903946\n"
     ]
    }
   ],
   "source": [
    "run_test(Xlda_train, ylda_train, 5, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB:\n",
      "[0.57021277 0.58091286 0.67153285 0.55506608 0.6124031 ] 0.5980255311627326\n",
      "DecisionTree\n",
      "[0.55793991 0.52252252 0.54098361 0.59349593 0.53556485] 0.5501013663517649\n",
      "SVM:\n",
      "[0.74893617 0.64912281 0.73170732 0.71861472 0.64102564] 0.6978813307887679\n",
      "XGB Default:\n",
      "[0.72268908 0.60082305 0.64777328 0.68085106 0.62337662] 0.6551026174912759\n"
     ]
    }
   ],
   "source": [
    "run_test(Xa_train, ya_train, 5, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=0.2, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=8, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "svm_clf = svm.SVC(random_state=8)\n",
    "svm_params = {\n",
    "    \"kernel\":['rbf','linear'],\n",
    "    'C':[0.1,0.2,0.4,0.6,0.8,1,10],\n",
    "    'gamma': np.logspace(-1,1,9)\n",
    "}\n",
    "scorer = make_scorer(fbeta_score,beta=0.5)\n",
    "svm_gs = GridSearchCV(svm_clf, svm_params, cv=5, scoring=scorer, n_jobs=-1)\n",
    "svm_gs.fit(Xa_train,ya_train)\n",
    "best_clf = svm_gs.best_estimator_\n",
    "print(best_clf)\n",
    "best_pred = best_clf.predict(Xa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6975, 'recall': 0.7027027027027027, 'precision': 0.6632653061224489}\n"
     ]
    }
   ],
   "source": [
    "performance = {'accuracy': accuracy_score(best_pred,y_test),\n",
    "                'recall': recall_score(best_pred,y_test),\n",
    "                'precision': precision_score(best_pred,y_test)}\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
